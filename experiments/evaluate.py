"""
HD-DDPG Evaluation Script - é«˜åº¦å¢å¼ºç‰ˆæœ¬
HD-DDPGæ¨¡å‹è¯„ä¼°è„šæœ¬ - ä¸“ä¸ºç¨³å®šåŒ–ç»„ä»¶è®¾è®¡

ğŸ¯ ä¸»è¦å¢å¼ºï¼š
- é›†æˆç¨³å®šåŒ–ç»„ä»¶
- å®æ—¶è¿›åº¦æ˜¾ç¤º
- å¢å¼ºç»Ÿè®¡åˆ†æ
- è¯¦ç»†å¯è§†åŒ–
- æ€§èƒ½åŸºå‡†æ¯”è¾ƒ
"""

import os
import sys
import argparse
import json
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ğŸ¯ å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
try:
    from tqdm import tqdm, trange
    TQDM_AVAILABLE = True
except ImportError:
    print("âš ï¸ tqdmæœªå®‰è£…ï¼Œå»ºè®®å®‰è£…: pip install tqdm")
    TQDM_AVAILABLE = False
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable
            self.total = total
            self.desc = desc
            self.n = 0
        def __iter__(self):
            for item in self.iterable:
                yield item
                self.update(1)
        def update(self, n=1):
            self.n += n
        def set_postfix(self, **kwargs):
            pass

# å¯¼å…¥ç»Ÿè®¡åˆ†æåŒ…
try:
    from scipy import stats
    import scipy.stats as scipy_stats
    SCIPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ scipyæœªå®‰è£…ï¼Œéƒ¨åˆ†ç»Ÿè®¡åŠŸèƒ½å°†ä¸å¯ç”¨")
    SCIPY_AVAILABLE = False

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ç¨³å®šåŒ–ç»„ä»¶
try:
    from src.algorithms.hd_ddpg import HDDDPG
    from src.environment.medical_simulator import StabilizedMedicalSchedulingSimulator
    from src.environment.fog_cloud_env import StabilizedFogCloudEnvironment
    from src.environment.workflow_generator import MedicalWorkflowGenerator
    from src.utils.metrics import EnhancedSchedulingMetrics
    STABILIZED_MODULES = True
    print("âœ… ç¨³å®šåŒ–ç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ç¨³å®šåŒ–ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸ å°†ä½¿ç”¨æ ‡å‡†ç»„ä»¶")
    try:
        from src.algorithms.hd_ddpg import HDDDPG
        from src.environment.medical_simulator import MedicalSchedulingSimulator
        from src.environment.fog_cloud_env import FogCloudEnvironment
        from src.environment.workflow_generator import MedicalWorkflowGenerator
        STABILIZED_MODULES = False
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥ä»»ä½•ç»„ä»¶")
        STABILIZED_MODULES = None


def parse_enhanced_arguments():
    """ğŸ¯ è§£æå¢å¼ºçš„å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='HD-DDPG Model Evaluation (é«˜åº¦å¢å¼ºç‰ˆæœ¬)',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # ğŸ”§ æ¨¡å‹å‚æ•°
    model_group = parser.add_argument_group('æ¨¡å‹å‚æ•°')
    model_group.add_argument('--model-path', type=str, required=True,
                            help='è®­ç»ƒæ¨¡å‹è·¯å¾„')
    model_group.add_argument('--config-path', type=str, default=None,
                            help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    model_group.add_argument('--model-type', type=str, choices=['standard', 'stabilized'],
                            default='stabilized', help='æ¨¡å‹ç±»å‹')

    # ğŸ”§ è¯„ä¼°å‚æ•°
    eval_group = parser.add_argument_group('è¯„ä¼°å‚æ•°')
    eval_group.add_argument('--test-workflows', type=int, default=200,
                           help='æµ‹è¯•å·¥ä½œæµæ•°é‡')
    eval_group.add_argument('--workflow-sizes', type=str, default='6,8,10,12,15,18,20',
                           help='æµ‹è¯•å·¥ä½œæµå¤§å°ï¼ˆé€—å·åˆ†éš”ï¼‰')
    eval_group.add_argument('--workflow-types', type=str,
                           default='radiology,pathology,general,surgery,emergency',
                           help='å·¥ä½œæµç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼‰')
    eval_group.add_argument('--evaluation-rounds', type=int, default=3,
                           help='è¯„ä¼°è½®æ•°ï¼ˆç”¨äºå‡å°‘éšæœºæ€§ï¼‰')

    # ğŸ”§ æ¯”è¾ƒå‚æ•°
    comp_group = parser.add_argument_group('åŸºå‡†æ¯”è¾ƒ')
    comp_group.add_argument('--baseline-algorithms', type=str,
                           default='HEFT,FCFS,Random,RoundRobin,MinMin',
                           help='åŸºçº¿ç®—æ³•ï¼ˆé€—å·åˆ†éš”ï¼‰')
    comp_group.add_argument('--include-statistical-test', action='store_true', default=True,
                           help='æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ')
    comp_group.add_argument('--confidence-level', type=float, default=0.95,
                           help='ç½®ä¿¡æ°´å¹³')

    # ğŸ”§ ç¯å¢ƒå‚æ•°
    env_group = parser.add_argument_group('ç¯å¢ƒå‚æ•°')
    env_group.add_argument('--failure-rate', type=float, default=0.005,
                          help='ç³»ç»Ÿæ•…éšœç‡')
    env_group.add_argument('--network-latency-factor', type=float, default=1.0,
                          help='ç½‘ç»œå»¶è¿Ÿå› å­')
    env_group.add_argument('--resource-heterogeneity', type=float, default=0.2,
                          help='èµ„æºå¼‚æ„æ€§ç¨‹åº¦')

    # ğŸ”§ è¾“å‡ºå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºå‚æ•°')
    output_group.add_argument('--output-dir', type=str, default='results/enhanced_evaluation',
                             help='è¾“å‡ºç›®å½•')
    output_group.add_argument('--save-plots', action='store_true', default=True,
                             help='ä¿å­˜è¯„ä¼°å›¾è¡¨')
    output_group.add_argument('--save-detailed-results', action='store_true', default=True,
                             help='ä¿å­˜è¯¦ç»†ç»“æœ')
    output_group.add_argument('--export-format', type=str, choices=['csv', 'json', 'both'],
                             default='both', help='å¯¼å‡ºæ ¼å¼')

    # ğŸ”§ å¯è§†åŒ–å‚æ•°
    viz_group = parser.add_argument_group('å¯è§†åŒ–å‚æ•°')
    viz_group.add_argument('--plot-style', type=str, choices=['seaborn', 'matplotlib', 'ggplot'],
                          default='seaborn', help='ç»˜å›¾é£æ ¼')
    viz_group.add_argument('--figure-dpi', type=int, default=300,
                          help='å›¾è¡¨DPI')
    viz_group.add_argument('--disable-progress-bar', action='store_true',
                          help='ç¦ç”¨è¿›åº¦æ¡')

    # ğŸ”§ åˆ†æå‚æ•°
    analysis_group = parser.add_argument_group('åˆ†æå‚æ•°')
    analysis_group.add_argument('--performance-metrics', type=str,
                               default='makespan,load_balance,energy,stability,quality',
                               help='æ€§èƒ½æŒ‡æ ‡ï¼ˆé€—å·åˆ†éš”ï¼‰')
    analysis_group.add_argument('--detailed-analysis', action='store_true', default=True,
                               help='æ‰§è¡Œè¯¦ç»†åˆ†æ')
    analysis_group.add_argument('--scalability-analysis', action='store_true', default=True,
                               help='å¯æ‰©å±•æ€§åˆ†æ')

    # é€šç”¨å‚æ•°
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')

    return parser.parse_args()


def load_enhanced_model_and_config(model_path, config_path=None, model_type='stabilized'):
    """ğŸ¯ åŠ è½½å¢å¼ºæ¨¡å‹å’Œé…ç½®"""
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {model_path}")

    # ğŸ¯ æ™ºèƒ½é…ç½®æ–‡ä»¶æŸ¥æ‰¾
    if config_path is None:
        potential_config_paths = [
            os.path.join(os.path.dirname(model_path), '..', 'config.json'),
            os.path.join(os.path.dirname(model_path), '..', '..', 'config.json'),
            os.path.join(os.path.dirname(model_path), 'config.json'),
            'config.json',
            'stabilized_config.json'
        ]

        for path in potential_config_paths:
            if os.path.exists(path):
                config_path = path
                break

    # åŠ è½½é…ç½®
    config = {}
    if config_path and os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"âœ… é…ç½®å·²åŠ è½½: {config_path}")
    else:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤é…ç½®")

    # ğŸ¯ åˆ›å»ºå¢å¼ºçš„HD-DDPGé…ç½®
    hd_ddpg_config = {
        # åŸºç¡€å‚æ•°
        'meta_state_dim': 15,
        'meta_action_dim': 3,
        'gamma': 0.95,
        'batch_size': config.get('batch_size', 16),
        'memory_capacity': config.get('memory_capacity', 8000),
        'meta_lr': config.get('learning_rate', 0.00008),
        'sub_lr': config.get('learning_rate', 0.00008),
        'tau': 0.005,
        'update_frequency': 1,
        'save_frequency': 50,

        # ğŸ¯ ç¨³å®šåŒ–å‚æ•°ï¼ˆå¦‚æœæ˜¯ç¨³å®šåŒ–æ¨¡å‹ï¼‰
        'gradient_clip_norm': 0.5,
        'exploration_noise': 0.0,  # è¯„ä¼°æ—¶ä¸ä½¿ç”¨æ¢ç´¢å™ªå£°
        'noise_decay': 1.0,
        'min_noise': 0.0,
        'reward_scale': 1.0,
        'verbose': False,

        # ğŸ¯ é«˜çº§å‚æ•°
        'action_smoothing': config.get('action_smoothing', True),
        'action_smoothing_alpha': 0.25,
        'makespan_weight': 0.75,
        'stability_weight': 0.15,
        'target_update_frequency': 3,

        # ğŸ¯ ç¨³å®šåŒ–ç‰¹æ€§
        'enable_per': config.get('enable_per', True),
        'quality_threshold': config.get('quality_threshold', 0.15),
        'td_error_clipping': True,
        'adaptive_learning': False,  # è¯„ä¼°æ—¶ç¦ç”¨
    }

    # åˆ›å»ºHD-DDPGå®ä¾‹
    try:
        hd_ddpg = HDDDPG(hd_ddpg_config)
        print("âœ… HD-DDPGå®ä¾‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ HD-DDPGåˆ›å»ºå¤±è´¥: {e}")
        return None, None

    # åŠ è½½æ¨¡å‹æƒé‡
    try:
        hd_ddpg.load_models(model_path)
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")

        # ğŸ¯ éªŒè¯æ¨¡å‹
        print("ğŸ” éªŒè¯æ¨¡å‹å®Œæ•´æ€§...")
        validation_result = validate_model(hd_ddpg)
        if validation_result:
            print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸ æ¨¡å‹éªŒè¯æœ‰è­¦å‘Šï¼Œä½†å°†ç»§ç»­è¯„ä¼°")

        return hd_ddpg, config

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None


def validate_model(hd_ddpg):
    """ğŸ¯ éªŒè¯æ¨¡å‹å®Œæ•´æ€§"""
    try:
        # æµ‹è¯•æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½
        test_state = np.random.random(15).astype(np.float32)

        # æµ‹è¯•å…ƒæ§åˆ¶å™¨
        cluster, action_probs = hd_ddpg.meta_controller.select_cluster(test_state)
        if cluster not in ['FPGA', 'FOG_GPU', 'CLOUD']:
            print(f"âš ï¸ å…ƒæ§åˆ¶å™¨è¾“å‡ºå¼‚å¸¸é›†ç¾¤: {cluster}")
            return False

        # æµ‹è¯•å­æ§åˆ¶å™¨
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            state_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
            test_cluster_state = np.random.random(state_dims[cluster_type]).astype(np.float32)

            # åˆ›å»ºæ¨¡æ‹ŸèŠ‚ç‚¹
            mock_nodes = [MockNode(i, cluster_type) for i in range(2)]

            try:
                node, node_probs = hd_ddpg.sub_controller_manager.select_node_in_cluster(
                    cluster_type, test_cluster_state, mock_nodes
                )
                if node is None:
                    print(f"âš ï¸ {cluster_type}å­æ§åˆ¶å™¨æœªé€‰æ‹©èŠ‚ç‚¹")
            except Exception as e:
                print(f"âš ï¸ {cluster_type}å­æ§åˆ¶å™¨æµ‹è¯•å¤±è´¥: {e}")
                return False

        print("âœ… æ‰€æœ‰ç»„ä»¶éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹éªŒè¯é”™è¯¯: {e}")
        return False


class MockNode:
    """æ¨¡æ‹ŸèŠ‚ç‚¹ç±»ç”¨äºéªŒè¯"""
    def __init__(self, node_id, cluster_type):
        self.node_id = node_id
        self.cluster_type = cluster_type
        self.memory_capacity = 100
        self.current_load = 0
        self.availability = True


def create_enhanced_environment(args):
    """ğŸ¯ åˆ›å»ºå¢å¼ºçš„è¯„ä¼°ç¯å¢ƒ"""
    try:
        if STABILIZED_MODULES:
            # ä½¿ç”¨ç¨³å®šåŒ–ç¯å¢ƒ
            env_config = {
                'failure_rate': args.failure_rate,
                'network_latency_factor': args.network_latency_factor,
                'resource_heterogeneity': args.resource_heterogeneity,
                'stability_monitoring': True,
                'performance_tracking': True
            }
            environment = StabilizedFogCloudEnvironment(env_config)
            print("âœ… ç¨³å®šåŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        else:
            # ä½¿ç”¨æ ‡å‡†ç¯å¢ƒ
            environment = FogCloudEnvironment()
            print("âœ… æ ‡å‡†ç¯å¢ƒåˆ›å»ºæˆåŠŸ")

        return environment

    except Exception as e:
        print(f"âš ï¸ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
        # åˆ›å»ºç®€åŒ–ç¯å¢ƒ
        return create_fallback_environment()


def create_fallback_environment():
    """åˆ›å»ºå›é€€ç¯å¢ƒ"""
    class FallbackEnvironment:
        def __init__(self):
            self.current_time = 0
            self.nodes = self._create_nodes()

        def _create_nodes(self):
            from collections import defaultdict
            nodes = defaultdict(list)

            node_configs = {
                'FPGA': {'count': 2, 'memory': 100, 'base_time': 0.4, 'base_energy': 8},
                'FOG_GPU': {'count': 3, 'memory': 150, 'base_time': 0.8, 'base_energy': 15},
                'CLOUD': {'count': 2, 'memory': 200, 'base_time': 1.6, 'base_energy': 25}
            }

            node_id = 0
            for cluster_type, config in node_configs.items():
                for i in range(config['count']):
                    node = type('Node', (), {
                        'node_id': node_id,
                        'cluster_type': cluster_type,
                        'memory_capacity': config['memory'],
                        'current_load': 0,
                        'availability': True,
                        'can_accommodate': lambda self, req: self.memory_capacity - self.current_load >= req,
                        'get_execution_time': lambda self, comp_req, cfg=config: cfg['base_time'] * comp_req,
                        'get_energy_consumption': lambda self, exec_time, cfg=config: cfg['base_energy'] * exec_time
                    })()
                    nodes[cluster_type].append(node)
                    node_id += 1

            return nodes

        def reset(self):
            self.current_time = 0
            for cluster_nodes in self.nodes.values():
                for node in cluster_nodes:
                    node.current_load = 0

        def get_system_state(self):
            return np.random.random(15)

        def get_stabilized_system_state(self):
            return self.get_system_state()

        def get_cluster_state(self, cluster_type):
            dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
            return np.random.random(dims.get(cluster_type, 6))

        def get_enhanced_cluster_state(self, cluster_type):
            return self.get_cluster_state(cluster_type)

        def get_available_nodes(self, cluster_type=None):
            if cluster_type:
                return [node for node in self.nodes.get(cluster_type, []) if node.availability]
            else:
                available = []
                for cluster_nodes in self.nodes.values():
                    available.extend([node for node in cluster_nodes if node.availability])
                return available

        def update_node_load(self, node_id, memory_req):
            for cluster_nodes in self.nodes.values():
                for node in cluster_nodes:
                    if node.node_id == node_id:
                        node.current_load += memory_req
                        break

    return FallbackEnvironment()


def generate_enhanced_test_workflows(workflow_generator, test_configs, args):
    """ğŸ¯ ç”Ÿæˆå¢å¼ºçš„æµ‹è¯•å·¥ä½œæµ"""
    print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•å·¥ä½œæµ...")

    test_workflows = []

    # è®¾ç½®è¿›åº¦æ¡
    use_tqdm = TQDM_AVAILABLE and not args.disable_progress_bar

    if use_tqdm:
        config_pbar = tqdm(test_configs, desc="ç”Ÿæˆå·¥ä½œæµé…ç½®", unit="config")
    else:
        config_pbar = test_configs

    for config in config_pbar:
        workflow_type = config['type']
        workflow_size = config['size']
        count = config['count']

        for round_idx in range(args.evaluation_rounds):
            for i in range(count):
                try:
                    if hasattr(workflow_generator, 'generate_workflow'):
                        workflow = workflow_generator.generate_workflow(
                            num_tasks=workflow_size,
                            workflow_type=workflow_type
                        )
                    else:
                        # åˆ›å»ºç®€åŒ–å·¥ä½œæµ
                        workflow = create_simple_workflow(workflow_size, workflow_type, f"{workflow_type}_{workflow_size}_{round_idx}_{i}")

                    test_workflows.append({
                        'workflow': workflow,
                        'type': workflow_type,
                        'size': workflow_size,
                        'round': round_idx,
                        'workflow_id': f"{workflow_type}_{workflow_size}_{round_idx}_{i}"
                    })

                except Exception as e:
                    print(f"âš ï¸ å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
                    # åˆ›å»ºç®€åŒ–å·¥ä½œæµä½œä¸ºå¤‡ç”¨
                    workflow = create_simple_workflow(workflow_size, workflow_type, f"fallback_{workflow_type}_{workflow_size}_{round_idx}_{i}")
                    test_workflows.append({
                        'workflow': workflow,
                        'type': workflow_type,
                        'size': workflow_size,
                        'round': round_idx,
                        'workflow_id': f"fallback_{workflow_type}_{workflow_size}_{round_idx}_{i}"
                    })

    print(f"âœ… ç”Ÿæˆäº† {len(test_workflows)} ä¸ªæµ‹è¯•å·¥ä½œæµ")
    return test_workflows


def create_simple_workflow(size, workflow_type, workflow_id):
    """åˆ›å»ºç®€åŒ–å·¥ä½œæµ"""
    class SimpleTask:
        def __init__(self, task_id, computation_req=1.0, memory_req=10, priority=1):
            self.task_id = task_id
            self.computation_requirement = computation_req
            self.memory_requirement = memory_req
            self.priority = priority
            self.dependencies = []

    workflow = []
    for i in range(size):
        # æ ¹æ®å·¥ä½œæµç±»å‹è°ƒæ•´ä»»åŠ¡å±æ€§
        if workflow_type == 'radiology':
            comp_req = np.random.uniform(1.0, 2.5)
            mem_req = np.random.randint(15, 35)
        elif workflow_type == 'pathology':
            comp_req = np.random.uniform(0.8, 2.0)
            mem_req = np.random.randint(10, 25)
        elif workflow_type == 'surgery':
            comp_req = np.random.uniform(1.5, 3.0)
            mem_req = np.random.randint(20, 40)
        elif workflow_type == 'emergency':
            comp_req = np.random.uniform(0.5, 1.5)
            mem_req = np.random.randint(8, 20)
        else:  # general
            comp_req = np.random.uniform(0.7, 2.0)
            mem_req = np.random.randint(10, 30)

        task = SimpleTask(
            task_id=f"{workflow_id}_task_{i}",
            computation_req=comp_req,
            memory_req=mem_req,
            priority=np.random.randint(1, 4)
        )
        workflow.append(task)

    return workflow


def evaluate_hd_ddpg_enhanced(hd_ddpg, test_workflows, environment, args):
    """ğŸ¯ å¢å¼ºçš„HD-DDPGè¯„ä¼°"""
    print(f"ğŸ¤– è¯„ä¼°HD-DDPG ({len(test_workflows)} ä¸ªå·¥ä½œæµ)...")

    results = []
    use_tqdm = TQDM_AVAILABLE and not args.disable_progress_bar

    if use_tqdm:
        workflow_pbar = tqdm(test_workflows, desc="HD-DDPGè¯„ä¼°", unit="workflow")
    else:
        workflow_pbar = test_workflows

    start_time = time.time()

    for i, test_case in enumerate(workflow_pbar):
        try:
            workflow = test_case['workflow']
            workflow_type = test_case['type']
            workflow_size = test_case['size']
            round_idx = test_case['round']
            workflow_id = test_case['workflow_id']

            # é‡ç½®ç¯å¢ƒ
            environment.reset()

            # ğŸ¯ è°ƒåº¦å·¥ä½œæµ
            schedule_start_time = time.time()
            result = hd_ddpg.schedule_workflow(workflow, environment)
            scheduling_time = time.time() - schedule_start_time

            # ğŸ¯ è®¡ç®—å¢å¼ºæŒ‡æ ‡
            enhanced_metrics = calculate_enhanced_metrics(result, workflow, environment)

            # è®°å½•ç»“æœ
            result_entry = {
                'workflow_id': workflow_id,
                'workflow_type': workflow_type,
                'workflow_size': workflow_size,
                'evaluation_round': round_idx,
                'makespan': result.get('makespan', float('inf')),
                'load_balance': result.get('load_balance', 0),
                'total_energy': result.get('total_energy', 0),
                'completed_tasks': result.get('completed_tasks', 0),
                'total_reward': result.get('total_reward', 0),
                'success_rate': result.get('success_rate', 0),
                'scheduling_time': scheduling_time,
                'algorithm': 'HD-DDPG',

                # ğŸ¯ å¢å¼ºæŒ‡æ ‡
                'stability_score': enhanced_metrics.get('stability_score', 0),
                'quality_score': enhanced_metrics.get('quality_score', 0),
                'resource_utilization': enhanced_metrics.get('resource_utilization', 0),
                'energy_efficiency': enhanced_metrics.get('energy_efficiency', 0),
                'throughput': enhanced_metrics.get('throughput', 0)
            }

            results.append(result_entry)

            # ğŸ¯ æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            if use_tqdm:
                workflow_pbar.set_postfix({
                    'Makespan': f"{result_entry['makespan']:.2f}",
                    'Success': f"{result_entry['success_rate']:.1%}",
                    'Quality': f"{result_entry['quality_score']:.2f}"
                })

        except Exception as e:
            print(f"âš ï¸ å·¥ä½œæµ {i} è¯„ä¼°å¤±è´¥: {e}")
            # æ·»åŠ å¤±è´¥è®°å½•
            results.append({
                'workflow_id': test_case.get('workflow_id', f'failed_{i}'),
                'workflow_type': test_case['type'],
                'workflow_size': test_case['size'],
                'evaluation_round': test_case['round'],
                'makespan': float('inf'),
                'load_balance': 0,
                'total_energy': 0,
                'completed_tasks': 0,
                'total_reward': -100,
                'success_rate': 0,
                'scheduling_time': 0,
                'algorithm': 'HD-DDPG',
                'stability_score': 0,
                'quality_score': 0,
                'resource_utilization': 0,
                'energy_efficiency': 0,
                'throughput': 0
            })

    total_time = time.time() - start_time
    print(f"âœ… HD-DDPGè¯„ä¼°å®Œæˆ (è€—æ—¶: {total_time:.2f}s)")

    return results


def calculate_enhanced_metrics(result, workflow, environment):
    """ğŸ¯ è®¡ç®—å¢å¼ºæŒ‡æ ‡"""
    try:
        enhanced_metrics = {}

        # ç¨³å®šæ€§è¯„åˆ†
        makespan = result.get('makespan', float('inf'))
        if makespan != float('inf'):
            # åŸºäºmakespançš„ç¨³å®šæ€§
            stability_score = 1.0 / (1.0 + makespan * 0.1)
        else:
            stability_score = 0.0
        enhanced_metrics['stability_score'] = stability_score

        # è´¨é‡è¯„åˆ†
        success_rate = result.get('success_rate', 0)
        load_balance = result.get('load_balance', 0)
        quality_score = (success_rate + load_balance + stability_score) / 3
        enhanced_metrics['quality_score'] = quality_score

        # èµ„æºåˆ©ç”¨ç‡
        completed_tasks = result.get('completed_tasks', 0)
        total_tasks = len(workflow) if workflow else 1
        resource_utilization = completed_tasks / total_tasks
        enhanced_metrics['resource_utilization'] = resource_utilization

        # èƒ½è€—æ•ˆç‡
        total_energy = result.get('total_energy', 1)
        if total_energy > 0 and completed_tasks > 0:
            energy_efficiency = completed_tasks / total_energy
        else:
            energy_efficiency = 0
        enhanced_metrics['energy_efficiency'] = energy_efficiency

        # ååé‡
        if makespan != float('inf') and makespan > 0:
            throughput = completed_tasks / makespan
        else:
            throughput = 0
        enhanced_metrics['throughput'] = throughput

        return enhanced_metrics

    except Exception as e:
        print(f"âš ï¸ å¢å¼ºæŒ‡æ ‡è®¡ç®—é”™è¯¯: {e}")
        return {
            'stability_score': 0,
            'quality_score': 0,
            'resource_utilization': 0,
            'energy_efficiency': 0,
            'throughput': 0
        }


def evaluate_baseline_enhanced(algorithm, test_workflows, environment, args):
    """ğŸ¯ å¢å¼ºçš„åŸºçº¿ç®—æ³•è¯„ä¼°"""
    print(f"ğŸ“Š è¯„ä¼°åŸºçº¿ç®—æ³•: {algorithm} ({len(test_workflows)} ä¸ªå·¥ä½œæµ)...")

    results = []
    use_tqdm = TQDM_AVAILABLE and not args.disable_progress_bar

    if use_tqdm:
        workflow_pbar = tqdm(test_workflows, desc=f"{algorithm}è¯„ä¼°", unit="workflow")
    else:
        workflow_pbar = test_workflows

    for i, test_case in enumerate(workflow_pbar):
        try:
            workflow = test_case['workflow']
            workflow_type = test_case['type']
            workflow_size = test_case['size']
            round_idx = test_case['round']
            workflow_id = test_case['workflow_id']

            # é‡ç½®ç¯å¢ƒ
            environment.reset()

            # ğŸ¯ è¿è¡ŒåŸºçº¿ç®—æ³•
            schedule_start_time = time.time()
            result = run_baseline_algorithm(algorithm, workflow, environment)
            scheduling_time = time.time() - schedule_start_time

            # ğŸ¯ è®¡ç®—å¢å¼ºæŒ‡æ ‡
            enhanced_metrics = calculate_enhanced_metrics(result, workflow, environment)

            # è®°å½•ç»“æœ
            result_entry = {
                'workflow_id': workflow_id,
                'workflow_type': workflow_type,
                'workflow_size': workflow_size,
                'evaluation_round': round_idx,
                'makespan': result.get('makespan', float('inf')),
                'load_balance': result.get('load_balance', 0),
                'total_energy': result.get('total_energy', 0),
                'completed_tasks': result.get('completed_tasks', 0),
                'total_reward': 0,  # åŸºçº¿ç®—æ³•ä¸è®¡ç®—reward
                'success_rate': result.get('success_rate', 0),
                'scheduling_time': scheduling_time,
                'algorithm': algorithm,

                # ğŸ¯ å¢å¼ºæŒ‡æ ‡
                'stability_score': enhanced_metrics.get('stability_score', 0),
                'quality_score': enhanced_metrics.get('quality_score', 0),
                'resource_utilization': enhanced_metrics.get('resource_utilization', 0),
                'energy_efficiency': enhanced_metrics.get('energy_efficiency', 0),
                'throughput': enhanced_metrics.get('throughput', 0)
            }

            results.append(result_entry)

            # ğŸ¯ æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            if use_tqdm:
                workflow_pbar.set_postfix({
                    'Makespan': f"{result_entry['makespan']:.2f}",
                    'Success': f"{result_entry['success_rate']:.1%}"
                })

        except Exception as e:
            print(f"âš ï¸ {algorithm} å·¥ä½œæµ {i} è¯„ä¼°å¤±è´¥: {e}")
            # æ·»åŠ å¤±è´¥è®°å½•
            results.append({
                'workflow_id': test_case.get('workflow_id', f'{algorithm}_failed_{i}'),
                'workflow_type': test_case['type'],
                'workflow_size': test_case['size'],
                'evaluation_round': test_case['round'],
                'makespan': float('inf'),
                'load_balance': 0,
                'total_energy': 0,
                'completed_tasks': 0,
                'total_reward': 0,
                'success_rate': 0,
                'scheduling_time': 0,
                'algorithm': algorithm,
                'stability_score': 0,
                'quality_score': 0,
                'resource_utilization': 0,
                'energy_efficiency': 0,
                'throughput': 0
            })

    print(f"âœ… {algorithm}è¯„ä¼°å®Œæˆ")
    return results


def run_baseline_algorithm(algorithm, workflow, environment):
    """ğŸ¯ è¿è¡ŒåŸºçº¿ç®—æ³•"""
    try:
        if algorithm == 'HEFT':
            return run_heft_algorithm(workflow, environment)
        elif algorithm == 'FCFS':
            return run_fcfs_algorithm(workflow, environment)
        elif algorithm == 'Random':
            return run_random_algorithm(workflow, environment)
        elif algorithm == 'RoundRobin':
            return run_round_robin_algorithm(workflow, environment)
        elif algorithm == 'MinMin':
            return run_min_min_algorithm(workflow, environment)
        else:
            print(f"âš ï¸ æœªçŸ¥åŸºçº¿ç®—æ³•: {algorithm}")
            return run_random_algorithm(workflow, environment)

    except Exception as e:
        print(f"âš ï¸ åŸºçº¿ç®—æ³• {algorithm} æ‰§è¡Œå¤±è´¥: {e}")
        return {
            'makespan': float('inf'),
            'load_balance': 0,
            'total_energy': 0,
            'completed_tasks': 0,
            'success_rate': 0
        }


def run_heft_algorithm(workflow, environment):
    """HEFTç®—æ³•å®ç°"""
    try:
        # ç®€åŒ–çš„HEFTå®ç°
        task_completion_times = []
        total_energy = 0
        completed_tasks = 0

        # è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
        all_nodes = []
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            nodes = environment.get_available_nodes(cluster_type)
            all_nodes.extend(nodes)

        if not all_nodes:
            return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
                   'completed_tasks': 0, 'success_rate': 0}

        # æŒ‰è®¡ç®—æ—¶é—´æ’åºä»»åŠ¡
        for task in workflow:
            # é€‰æ‹©æœ€å¿«çš„èŠ‚ç‚¹
            best_node = None
            best_time = float('inf')

            for node in all_nodes:
                if node.availability and hasattr(node, 'get_execution_time'):
                    comp_req = getattr(task, 'computation_requirement', 1.0)
                    exec_time = node.get_execution_time(comp_req)
                    if exec_time < best_time:
                        best_time = exec_time
                        best_node = node

            if best_node:
                # æ‰§è¡Œä»»åŠ¡
                completion_time = environment.current_time + best_time
                task_completion_times.append(completion_time)

                # è®¡ç®—èƒ½è€—
                if hasattr(best_node, 'get_energy_consumption'):
                    energy = best_node.get_energy_consumption(best_time)
                    total_energy += energy

                # æ›´æ–°ç¯å¢ƒ
                environment.current_time = completion_time
                memory_req = getattr(task, 'memory_requirement', 10)
                environment.update_node_load(best_node.node_id, memory_req)
                completed_tasks += 1

        # è®¡ç®—æŒ‡æ ‡
        makespan = max(task_completion_times) if task_completion_times else float('inf')
        success_rate = completed_tasks / len(workflow) if workflow else 0

        # ç®€åŒ–çš„è´Ÿè½½å‡è¡¡è®¡ç®—
        load_balance = min(1.0, success_rate)

        return {
            'makespan': makespan,
            'load_balance': load_balance,
            'total_energy': total_energy,
            'completed_tasks': completed_tasks,
            'success_rate': success_rate
        }

    except Exception as e:
        print(f"âš ï¸ HEFTç®—æ³•æ‰§è¡Œé”™è¯¯: {e}")
        return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
               'completed_tasks': 0, 'success_rate': 0}


def run_fcfs_algorithm(workflow, environment):
    """FCFSç®—æ³•å®ç°"""
    try:
        task_completion_times = []
        total_energy = 0
        completed_tasks = 0

        # è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
        all_nodes = []
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            nodes = environment.get_available_nodes(cluster_type)
            all_nodes.extend(nodes)

        if not all_nodes:
            return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
                   'completed_tasks': 0, 'success_rate': 0}

        node_index = 0

        # æŒ‰é¡ºåºåˆ†é…ä»»åŠ¡
        for task in workflow:
            if all_nodes:
                # è½®æµé€‰æ‹©èŠ‚ç‚¹
                selected_node = all_nodes[node_index % len(all_nodes)]
                node_index += 1

                if hasattr(selected_node, 'get_execution_time'):
                    comp_req = getattr(task, 'computation_requirement', 1.0)
                    exec_time = selected_node.get_execution_time(comp_req)

                    completion_time = environment.current_time + exec_time
                    task_completion_times.append(completion_time)

                    # è®¡ç®—èƒ½è€—
                    if hasattr(selected_node, 'get_energy_consumption'):
                        energy = selected_node.get_energy_consumption(exec_time)
                        total_energy += energy

                    # æ›´æ–°ç¯å¢ƒ
                    environment.current_time = completion_time
                    memory_req = getattr(task, 'memory_requirement', 10)
                    environment.update_node_load(selected_node.node_id, memory_req)
                    completed_tasks += 1

        # è®¡ç®—æŒ‡æ ‡
        makespan = max(task_completion_times) if task_completion_times else float('inf')
        success_rate = completed_tasks / len(workflow) if workflow else 0
        load_balance = min(1.0, success_rate)

        return {
            'makespan': makespan,
            'load_balance': load_balance,
            'total_energy': total_energy,
            'completed_tasks': completed_tasks,
            'success_rate': success_rate
        }

    except Exception as e:
        print(f"âš ï¸ FCFSç®—æ³•æ‰§è¡Œé”™è¯¯: {e}")
        return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
               'completed_tasks': 0, 'success_rate': 0}


def run_random_algorithm(workflow, environment):
    """éšæœºç®—æ³•å®ç°"""
    try:
        task_completion_times = []
        total_energy = 0
        completed_tasks = 0

        # è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
        all_nodes = []
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            nodes = environment.get_available_nodes(cluster_type)
            all_nodes.extend(nodes)

        if not all_nodes:
            return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
                   'completed_tasks': 0, 'success_rate': 0}

        # éšæœºåˆ†é…ä»»åŠ¡
        for task in workflow:
            if all_nodes:
                # éšæœºé€‰æ‹©èŠ‚ç‚¹
                selected_node = np.random.choice(all_nodes)

                if hasattr(selected_node, 'get_execution_time'):
                    comp_req = getattr(task, 'computation_requirement', 1.0)
                    exec_time = selected_node.get_execution_time(comp_req)

                    completion_time = environment.current_time + exec_time
                    task_completion_times.append(completion_time)

                    # è®¡ç®—èƒ½è€—
                    if hasattr(selected_node, 'get_energy_consumption'):
                        energy = selected_node.get_energy_consumption(exec_time)
                        total_energy += energy

                    # æ›´æ–°ç¯å¢ƒ
                    environment.current_time = completion_time
                    memory_req = getattr(task, 'memory_requirement', 10)
                    environment.update_node_load(selected_node.node_id, memory_req)
                    completed_tasks += 1

        # è®¡ç®—æŒ‡æ ‡
        makespan = max(task_completion_times) if task_completion_times else float('inf')
        success_rate = completed_tasks / len(workflow) if workflow else 0
        load_balance = min(1.0, success_rate * 0.7)  # éšæœºç®—æ³•è´Ÿè½½å‡è¡¡è¾ƒå·®

        return {
            'makespan': makespan,
            'load_balance': load_balance,
            'total_energy': total_energy,
            'completed_tasks': completed_tasks,
            'success_rate': success_rate
        }

    except Exception as e:
        print(f"âš ï¸ Randomç®—æ³•æ‰§è¡Œé”™è¯¯: {e}")
        return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
               'completed_tasks': 0, 'success_rate': 0}


def run_round_robin_algorithm(workflow, environment):
    """è½®è¯¢ç®—æ³•å®ç°"""
    # ä¸FCFSç±»ä¼¼ï¼Œä½†æ›´è§„åˆ™çš„è½®è¯¢
    return run_fcfs_algorithm(workflow, environment)


def run_min_min_algorithm(workflow, environment):
    """MinMinç®—æ³•å®ç°"""
    try:
        task_completion_times = []
        total_energy = 0
        completed_tasks = 0

        # è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
        all_nodes = []
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            nodes = environment.get_available_nodes(cluster_type)
            all_nodes.extend(nodes)

        if not all_nodes:
            return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
                   'completed_tasks': 0, 'success_rate': 0}

        # MinMin: ä¼˜å…ˆå¤„ç†å°ä»»åŠ¡
        sorted_workflow = sorted(workflow,
                               key=lambda t: getattr(t, 'computation_requirement', 1.0))

        for task in sorted_workflow:
            # é€‰æ‹©æœ€å¿«çš„èŠ‚ç‚¹
            best_node = None
            best_time = float('inf')

            for node in all_nodes:
                if node.availability and hasattr(node, 'get_execution_time'):
                    comp_req = getattr(task, 'computation_requirement', 1.0)
                    exec_time = node.get_execution_time(comp_req)
                    if exec_time < best_time:
                        best_time = exec_time
                        best_node = node

            if best_node:
                completion_time = environment.current_time + best_time
                task_completion_times.append(completion_time)

                # è®¡ç®—èƒ½è€—
                if hasattr(best_node, 'get_energy_consumption'):
                    energy = best_node.get_energy_consumption(best_time)
                    total_energy += energy

                # æ›´æ–°ç¯å¢ƒ
                environment.current_time = completion_time
                memory_req = getattr(task, 'memory_requirement', 10)
                environment.update_node_load(best_node.node_id, memory_req)
                completed_tasks += 1

        # è®¡ç®—æŒ‡æ ‡
        makespan = max(task_completion_times) if task_completion_times else float('inf')
        success_rate = completed_tasks / len(workflow) if workflow else 0
        load_balance = min(1.0, success_rate * 0.8)  # MinMinè´Ÿè½½å‡è¡¡ä¸€èˆ¬

        return {
            'makespan': makespan,
            'load_balance': load_balance,
            'total_energy': total_energy,
            'completed_tasks': completed_tasks,
            'success_rate': success_rate
        }

    except Exception as e:
        print(f"âš ï¸ MinMinç®—æ³•æ‰§è¡Œé”™è¯¯: {e}")
        return {'makespan': float('inf'), 'load_balance': 0, 'total_energy': 0,
               'completed_tasks': 0, 'success_rate': 0}


def perform_enhanced_statistical_analysis(results_df, args):
    """ğŸ¯ æ‰§è¡Œå¢å¼ºçš„ç»Ÿè®¡åˆ†æ"""
    if not SCIPY_AVAILABLE:
        print("âš ï¸ scipyä¸å¯ç”¨ï¼Œè·³è¿‡ç»Ÿè®¡åˆ†æ")
        return {}

    print("\n=== å¢å¼ºç»Ÿè®¡åˆ†æ ===")

    # æŒ‰ç®—æ³•åˆ†ç»„
    algorithms = results_df['algorithm'].unique()
    metrics = ['makespan', 'load_balance', 'total_energy', 'scheduling_time',
               'stability_score', 'quality_score', 'resource_utilization',
               'energy_efficiency', 'throughput']

    statistical_results = {}

    for metric in metrics:
        print(f"\nğŸ“Š {metric.upper()} åˆ†æ:")
        metric_results = {}

        # è®¡ç®—æè¿°æ€§ç»Ÿè®¡
        for algorithm in algorithms:
            data = results_df[results_df['algorithm'] == algorithm][metric]
            # è¿‡æ»¤æ— æ•ˆå€¼
            valid_data = data[~data.isin([float('inf'), float('-inf')]) & data.notna()]

            if len(valid_data) > 0:
                metric_results[algorithm] = {
                    'count': len(valid_data),
                    'mean': float(valid_data.mean()),
                    'std': float(valid_data.std()),
                    'median': float(valid_data.median()),
                    'min': float(valid_data.min()),
                    'max': float(valid_data.max()),
                    'q25': float(valid_data.quantile(0.25)),
                    'q75': float(valid_data.quantile(0.75))
                }

                print(f"  {algorithm:12s}: {valid_data.mean():8.3f} Â± {valid_data.std():6.3f} "
                      f"(n={len(valid_data)})")

        # HD-DDPG vs åŸºçº¿ç®—æ³•çš„æ˜¾è‘—æ€§æ£€éªŒ
        if 'HD-DDPG' in algorithms:
            hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG'][metric]
            hd_ddpg_valid = hd_ddpg_data[~hd_ddpg_data.isin([float('inf'), float('-inf')]) & hd_ddpg_data.notna()]

            for baseline in algorithms:
                if baseline != 'HD-DDPG':
                    baseline_data = results_df[results_df['algorithm'] == baseline][metric]
                    baseline_valid = baseline_data[~baseline_data.isin([float('inf'), float('-inf')]) & baseline_data.notna()]

                    if len(hd_ddpg_valid) > 5 and len(baseline_valid) > 5:
                        try:
                            # ğŸ¯ å¤šç§ç»Ÿè®¡æ£€éªŒ
                            # Wilcoxon rank-sum test (éå‚æ•°)
                            statistic_w, p_value_w = stats.ranksums(hd_ddpg_valid, baseline_valid)

                            # Mann-Whitney U test
                            statistic_u, p_value_u = stats.mannwhitneyu(hd_ddpg_valid, baseline_valid, alternative='two-sided')

                            # T-test (å¦‚æœæ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ)
                            try:
                                statistic_t, p_value_t = stats.ttest_ind(hd_ddpg_valid, baseline_valid, equal_var=False)
                            except:
                                statistic_t, p_value_t = np.nan, 1.0

                            # æ•ˆåº”å¤§å° (Cohen's d)
                            pooled_std = np.sqrt(((len(hd_ddpg_valid) - 1) * hd_ddpg_valid.var() +
                                                 (len(baseline_valid) - 1) * baseline_valid.var()) /
                                                (len(hd_ddpg_valid) + len(baseline_valid) - 2))
                            if pooled_std > 0:
                                cohens_d = (hd_ddpg_valid.mean() - baseline_valid.mean()) / pooled_std
                            else:
                                cohens_d = 0

                            metric_results[f'HD-DDPG_vs_{baseline}'] = {
                                'wilcoxon_statistic': float(statistic_w),
                                'wilcoxon_p_value': float(p_value_w),
                                'mannwhitney_statistic': float(statistic_u),
                                'mannwhitney_p_value': float(p_value_u),
                                'ttest_statistic': float(statistic_t),
                                'ttest_p_value': float(p_value_t),
                                'cohens_d': float(cohens_d),
                                'significant_wilcoxon': p_value_w < (1 - args.confidence_level),
                                'significant_mannwhitney': p_value_u < (1 - args.confidence_level),
                                'significant_ttest': p_value_t < (1 - args.confidence_level) if not np.isnan(p_value_t) else False,
                                'effect_size': 'large' if abs(cohens_d) > 0.8 else 'medium' if abs(cohens_d) > 0.5 else 'small'
                            }

                            # è¾“å‡ºç»“æœ
                            significance_w = "âœ“" if p_value_w < (1 - args.confidence_level) else "âœ—"
                            significance_u = "âœ“" if p_value_u < (1 - args.confidence_level) else "âœ—"
                            print(f"    vs {baseline:10s}: Wilcoxon p={p_value_w:.4f} {significance_w}, "
                                  f"M-W p={p_value_u:.4f} {significance_u}, Cohen's d={cohens_d:.3f}")

                        except Exception as e:
                            print(f"    vs {baseline:10s}: ç»Ÿè®¡æ£€éªŒå¤±è´¥ - {e}")

        statistical_results[metric] = metric_results

    return statistical_results


def create_enhanced_evaluation_plots(results_df, output_dir, args):
    """ğŸ¯ åˆ›å»ºå¢å¼ºçš„è¯„ä¼°å›¾è¡¨"""
    print("ğŸ“ˆ åˆ›å»ºå¢å¼ºè¯„ä¼°å›¾è¡¨...")

    # è®¾ç½®ç»˜å›¾é£æ ¼
    if args.plot_style == 'seaborn':
        plt.style.use('seaborn-v0_8')
    elif args.plot_style == 'ggplot':
        plt.style.use('ggplot')
    else:
        plt.style.use('default')

    # è®¾ç½®å­—ä½“å’Œé¢œè‰²
    plt.rcParams.update({
        'font.size': 12,
        'font.family': 'DejaVu Sans',
        'axes.grid': True,
        'grid.alpha': 0.3,
        'figure.autolayout': True
    })

    # ğŸ¯ ä¸»è¦æ€§èƒ½æ¯”è¾ƒå›¾ (3x3)
    fig1, axes1 = plt.subplots(3, 3, figsize=(20, 16))
    axes1 = axes1.flatten()

    main_metrics = ['makespan', 'load_balance', 'total_energy', 'scheduling_time',
                   'stability_score', 'quality_score', 'resource_utilization',
                   'energy_efficiency', 'throughput']

    algorithms = results_df['algorithm'].unique()

    for i, metric in enumerate(main_metrics):
        if i < len(axes1):
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            plot_data = results_df[~results_df[metric].isin([float('inf'), float('-inf')]) & results_df[metric].notna()]

            if len(plot_data) > 0:
                # ç®±çº¿å›¾
                sns.boxplot(data=plot_data, x='algorithm', y=metric, ax=axes1[i],
                           palette='Set2', showfliers=True)

                # æ·»åŠ å‡å€¼ç‚¹
                means = plot_data.groupby('algorithm')[metric].mean()
                for j, (algo, mean_val) in enumerate(means.items()):
                    axes1[i].scatter(j, mean_val, color='red', s=100, marker='D',
                                   zorder=10, label='Mean' if j == 0 else '')

                axes1[i].set_title(f'{metric.replace("_", " ").title()}', fontweight='bold', fontsize=14)
                axes1[i].tick_params(axis='x', rotation=45)
                axes1[i].grid(True, alpha=0.3)

                if i == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªå­å›¾æ·»åŠ å›¾ä¾‹
                    axes1[i].legend()
            else:
                axes1[i].text(0.5, 0.5, 'No Valid Data', ha='center', va='center',
                             transform=axes1[i].transAxes, fontsize=16)
                axes1[i].set_title(f'{metric.replace("_", " ").title()}', fontweight='bold')

    plt.suptitle('HD-DDPG vs Baseline Algorithms - Performance Comparison',
                fontsize=18, fontweight='bold')

    if args.save_plots:
        plot1_path = os.path.join(output_dir, 'enhanced_performance_comparison.png')
        plt.savefig(plot1_path, dpi=args.figure_dpi, bbox_inches='tight')
        print(f"âœ… æ€§èƒ½æ¯”è¾ƒå›¾å·²ä¿å­˜: {plot1_path}")

    plt.show()

    # ğŸ¯ å¯æ‰©å±•æ€§åˆ†æå›¾
    if args.scalability_analysis:
        fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))

        # å·¥ä½œæµå¤§å° vs æ€§èƒ½
        valid_data = results_df[~results_df['makespan'].isin([float('inf'), float('-inf')]) & results_df['makespan'].notna()]
        if len(valid_data) > 0:
            sns.lineplot(data=valid_data, x='workflow_size', y='makespan',
                        hue='algorithm', marker='o', ax=axes2[0,0])
            axes2[0,0].set_title('Makespan vs Workflow Size', fontweight='bold')
            axes2[0,0].grid(True, alpha=0.3)

        # å·¥ä½œæµç±»å‹æ€§èƒ½æ¯”è¾ƒ
        type_data = results_df[~results_df['makespan'].isin([float('inf'), float('-inf')]) & results_df['makespan'].notna()]
        if len(type_data) > 0:
            makespan_by_type = type_data.groupby(['algorithm', 'workflow_type'])['makespan'].mean().reset_index()
            sns.barplot(data=makespan_by_type, x='workflow_type', y='makespan',
                       hue='algorithm', ax=axes2[0,1])
            axes2[0,1].set_title('Average Makespan by Workflow Type', fontweight='bold')
            axes2[0,1].tick_params(axis='x', rotation=45)
            axes2[0,1].grid(True, alpha=0.3)

        # è´¨é‡è¯„åˆ†åˆ†å¸ƒ
        quality_data = results_df[results_df['quality_score'].notna()]
        if len(quality_data) > 0:
            sns.violinplot(data=quality_data, x='algorithm', y='quality_score', ax=axes2[1,0])
            axes2[1,0].set_title('Quality Score Distribution', fontweight='bold')
            axes2[1,0].tick_params(axis='x', rotation=45)
            axes2[1,0].grid(True, alpha=0.3)

        # èƒ½è€—æ•ˆç‡ vs ååé‡
        efficiency_data = results_df[(results_df['energy_efficiency'].notna()) &
                                   (results_df['throughput'].notna()) &
                                   (~results_df['energy_efficiency'].isin([float('inf'), float('-inf')])) &
                                   (~results_df['throughput'].isin([float('inf'), float('-inf')]))]
        if len(efficiency_data) > 0:
            sns.scatterplot(data=efficiency_data, x='energy_efficiency', y='throughput',
                           hue='algorithm', size='workflow_size', ax=axes2[1,1], alpha=0.7)
            axes2[1,1].set_title('Energy Efficiency vs Throughput', fontweight='bold')
            axes2[1,1].grid(True, alpha=0.3)

        plt.suptitle('Scalability and Detailed Performance Analysis',
                    fontsize=16, fontweight='bold')

        if args.save_plots:
            plot2_path = os.path.join(output_dir, 'scalability_analysis.png')
            plt.savefig(plot2_path, dpi=args.figure_dpi, bbox_inches='tight')
            print(f"âœ… å¯æ‰©å±•æ€§åˆ†æå›¾å·²ä¿å­˜: {plot2_path}")

        plt.show()

    # ğŸ¯ æ€§èƒ½æ”¹è¿›çƒ­å›¾
    if 'HD-DDPG' in algorithms:
        fig3, ax3 = plt.subplots(figsize=(12, 8))

        improvement_data = []

        for baseline in algorithms:
            if baseline != 'HD-DDPG':
                for metric in ['makespan', 'total_energy']:  # è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡
                    hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG'][metric]
                    baseline_data = results_df[results_df['algorithm'] == baseline][metric]

                    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                    hd_ddpg_valid = hd_ddpg_data[~hd_ddpg_data.isin([float('inf'), float('-inf')]) & hd_ddpg_data.notna()]
                    baseline_valid = baseline_data[~baseline_data.isin([float('inf'), float('-inf')]) & baseline_data.notna()]

                    if len(hd_ddpg_valid) > 0 and len(baseline_valid) > 0:
                        hd_ddpg_mean = hd_ddpg_valid.mean()
                        baseline_mean = baseline_valid.mean()
                        if baseline_mean > 0:
                            improvement = (baseline_mean - hd_ddpg_mean) / baseline_mean * 100
                            improvement_data.append({
                                'Baseline': baseline,
                                'Metric': metric,
                                'Improvement (%)': improvement
                            })

                # è´Ÿè½½å‡è¡¡ã€è´¨é‡è¯„åˆ†ç­‰ï¼ˆè¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡ï¼‰
                for metric in ['load_balance', 'quality_score', 'stability_score']:
                    hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG'][metric]
                    baseline_data = results_df[results_df['algorithm'] == baseline][metric]

                    hd_ddpg_valid = hd_ddpg_data[hd_ddpg_data.notna()]
                    baseline_valid = baseline_data[baseline_data.notna()]

                    if len(hd_ddpg_valid) > 0 and len(baseline_valid) > 0:
                        hd_ddpg_mean = hd_ddpg_valid.mean()
                        baseline_mean = baseline_valid.mean()
                        if baseline_mean > 0:
                            improvement = (hd_ddpg_mean - baseline_mean) / baseline_mean * 100
                            improvement_data.append({
                                'Baseline': baseline,
                                'Metric': metric,
                                'Improvement (%)': improvement
                            })

        if improvement_data:
            improvement_df = pd.DataFrame(improvement_data)
            improvement_pivot = improvement_df.pivot(index='Baseline', columns='Metric', values='Improvement (%)')

            # åˆ›å»ºçƒ­å›¾
            mask = improvement_pivot.isnull()
            sns.heatmap(improvement_pivot, annot=True, cmap='RdYlGn', center=0,
                       fmt='.1f', ax=ax3, mask=mask, cbar_kws={'label': 'Improvement (%)'})
            ax3.set_title('HD-DDPG Performance Improvement vs Baselines (%)',
                         fontweight='bold', fontsize=14)

            if args.save_plots:
                heatmap_path = os.path.join(output_dir, 'improvement_heatmap.png')
                plt.savefig(heatmap_path, dpi=args.figure_dpi, bbox_inches='tight')
                print(f"âœ… æ”¹è¿›çƒ­å›¾å·²ä¿å­˜: {heatmap_path}")

            plt.show()

    print("âœ… æ‰€æœ‰å›¾è¡¨åˆ›å»ºå®Œæˆ")


def generate_enhanced_evaluation_report(results_df, statistical_results, output_dir, args):
    """ğŸ¯ ç”Ÿæˆå¢å¼ºçš„è¯„ä¼°æŠ¥å‘Š"""
    print("ğŸ“ ç”Ÿæˆå¢å¼ºè¯„ä¼°æŠ¥å‘Š...")

    report_path = os.path.join(output_dir, 'enhanced_evaluation_report.md')

    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            # æŠ¥å‘Šå¤´éƒ¨
            f.write("# HD-DDPG Enhanced Evaluation Report\n")
            f.write("=" * 50 + "\n\n")

            f.write(f"**Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Total Test Workflows:** {len(results_df)}\n")
            f.write(f"**Evaluation Rounds:** {args.evaluation_rounds}\n")
            f.write(f"**Model Type:** {args.model_type}\n")
            f.write(f"**Statistical Confidence Level:** {args.confidence_level}\n\n")

            # ğŸ¯ æ‰§è¡Œæ‘˜è¦
            f.write("## Executive Summary\n")
            f.write("-" * 20 + "\n\n")

            # è®¡ç®—HD-DDPGçš„æ•´ä½“æ€§èƒ½
            hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG']
            if len(hd_ddpg_data) > 0:
                avg_makespan = hd_ddpg_data['makespan'][~hd_ddpg_data['makespan'].isin([float('inf')])].mean()
                avg_quality = hd_ddpg_data['quality_score'].mean()
                avg_stability = hd_ddpg_data['stability_score'].mean()
                success_rate = hd_ddpg_data['success_rate'].mean()

                f.write(f"**HD-DDPG Overall Performance:**\n")
                f.write(f"- Average Makespan: {avg_makespan:.3f}\n")
                f.write(f"- Quality Score: {avg_quality:.3f}\n")
                f.write(f"- Stability Score: {avg_stability:.3f}\n")
                f.write(f"- Success Rate: {success_rate:.1%}\n\n")

            # ğŸ¯ ç®—æ³•æ€§èƒ½æ‘˜è¦
            f.write("## Algorithm Performance Summary\n")
            f.write("-" * 30 + "\n\n")

            algorithms = results_df['algorithm'].unique()
            for algorithm in algorithms:
                algo_data = results_df[results_df['algorithm'] == algorithm]

                f.write(f"### {algorithm}\n")

                # åŸºç¡€æŒ‡æ ‡
                makespan_data = algo_data['makespan'][~algo_data['makespan'].isin([float('inf'), float('-inf')])].dropna()
                if len(makespan_data) > 0:
                    f.write(f"- **Makespan:** {makespan_data.mean():.3f} Â± {makespan_data.std():.3f}\n")

                for metric, name in [('load_balance', 'Load Balance'), ('total_energy', 'Energy'),
                                   ('scheduling_time', 'Scheduling Time'), ('quality_score', 'Quality Score'),
                                   ('stability_score', 'Stability Score')]:
                    data = algo_data[metric].dropna()
                    if len(data) > 0:
                        f.write(f"- **{name}:** {data.mean():.3f} Â± {data.std():.3f}\n")

                f.write("\n")

            # ğŸ¯ ç»Ÿè®¡æ˜¾è‘—æ€§ç»“æœ
            if statistical_results and args.include_statistical_test:
                f.write("## Statistical Significance Analysis\n")
                f.write("-" * 35 + "\n\n")

                for metric, metric_results in statistical_results.items():
                    f.write(f"### {metric.upper()}\n")

                    # æè¿°æ€§ç»Ÿè®¡
                    f.write("**Descriptive Statistics:**\n")
                    for algo, stats in metric_results.items():
                        if isinstance(stats, dict) and 'mean' in stats:
                            f.write(f"- {algo}: Î¼={stats['mean']:.3f}, Ïƒ={stats['std']:.3f}, "
                                   f"median={stats['median']:.3f} (n={stats['count']})\n")

                    f.write("\n**Statistical Tests (HD-DDPG vs Baselines):**\n")
                    for comparison, stats in metric_results.items():
                        if 'HD-DDPG_vs_' in comparison and isinstance(stats, dict):
                            baseline = comparison.replace('HD-DDPG_vs_', '')

                            # Wilcoxonç»“æœ
                            sig_w = "âœ“ Significant" if stats.get('significant_wilcoxon', False) else "âœ— Not Significant"
                            f.write(f"- **vs {baseline}:**\n")
                            f.write(f"  - Wilcoxon: p={stats.get('wilcoxon_p_value', 'N/A'):.4f} ({sig_w})\n")
                            f.write(f"  - Mann-Whitney: p={stats.get('mannwhitney_p_value', 'N/A'):.4f}\n")
                            f.write(f"  - Effect size (Cohen's d): {stats.get('cohens_d', 'N/A'):.3f} ({stats.get('effect_size', 'N/A')})\n")

                    f.write("\n")

            # ğŸ¯ å¯æ‰©å±•æ€§åˆ†æ
            if args.scalability_analysis:
                f.write("## Scalability Analysis\n")
                f.write("-" * 20 + "\n\n")

                # æŒ‰å·¥ä½œæµå¤§å°åˆ†æ
                f.write("### Performance by Workflow Size\n")
                for size in sorted(results_df['workflow_size'].unique()):
                    size_data = results_df[results_df['workflow_size'] == size]
                    f.write(f"**Size {size}:** {len(size_data)} workflows\n")

                    for algo in algorithms:
                        algo_size_data = size_data[size_data['algorithm'] == algo]
                        if len(algo_size_data) > 0:
                            makespan_data = algo_size_data['makespan'][~algo_size_data['makespan'].isin([float('inf')])].dropna()
                            if len(makespan_data) > 0:
                                f.write(f"- {algo}: {makespan_data.mean():.3f} Â± {makespan_data.std():.3f}\n")
                    f.write("\n")

                # æŒ‰å·¥ä½œæµç±»å‹åˆ†æ
                f.write("### Performance by Workflow Type\n")
                for wf_type in sorted(results_df['workflow_type'].unique()):
                    type_data = results_df[results_df['workflow_type'] == wf_type]
                    f.write(f"**{wf_type.title()}:** {len(type_data)} workflows\n")

                    for algo in algorithms:
                        algo_type_data = type_data[type_data['algorithm'] == algo]
                        if len(algo_type_data) > 0:
                            makespan_data = algo_type_data['makespan'][~algo_type_data['makespan'].isin([float('inf')])].dropna()
                            if len(makespan_data) > 0:
                                f.write(f"- {algo}: {makespan_data.mean():.3f} Â± {makespan_data.std():.3f}\n")
                    f.write("\n")

            # ğŸ¯ ç»“è®ºå’Œå»ºè®®
            f.write("## Conclusions and Recommendations\n")
            f.write("-" * 35 + "\n\n")

            # åŸºäºç»“æœç”Ÿæˆæ™ºèƒ½ç»“è®º
            if 'HD-DDPG' in algorithms and len(results_df) > 0:
                hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG']
                baseline_data = results_df[results_df['algorithm'] != 'HD-DDPG']

                if len(hd_ddpg_data) > 0 and len(baseline_data) > 0:
                    hd_ddpg_makespan = hd_ddpg_data['makespan'][~hd_ddpg_data['makespan'].isin([float('inf')])].mean()
                    baseline_makespan = baseline_data['makespan'][~baseline_data['makespan'].isin([float('inf')])].mean()

                    if hd_ddpg_makespan < baseline_makespan:
                        improvement_pct = (baseline_makespan - hd_ddpg_makespan) / baseline_makespan * 100
                        f.write(f"1. **HD-DDPG demonstrates superior performance** with {improvement_pct:.1f}% improvement in makespan compared to baseline algorithms.\n\n")

                    # ç¨³å®šæ€§è¯„ä¼°
                    hd_ddpg_stability = hd_ddpg_data['stability_score'].mean()
                    if hd_ddpg_stability > 0.7:
                        f.write("2. **High stability achieved** - HD-DDPG shows consistent performance across different workflow types and sizes.\n\n")
                    elif hd_ddpg_stability > 0.5:
                        f.write("2. **Moderate stability** - HD-DDPG shows reasonable consistency with room for improvement.\n\n")
                    else:
                        f.write("2. **Stability concerns** - HD-DDPG performance varies significantly across test cases.\n\n")

                    # è´¨é‡è¯„ä¼°
                    hd_ddpg_quality = hd_ddpg_data['quality_score'].mean()
                    if hd_ddpg_quality > 0.8:
                        f.write("3. **Excellent overall quality** - HD-DDPG delivers high-quality scheduling decisions.\n\n")
                    elif hd_ddpg_quality > 0.6:
                        f.write("3. **Good overall quality** - HD-DDPG provides satisfactory scheduling performance.\n\n")
                    else:
                        f.write("3. **Quality improvement needed** - HD-DDPG quality scores suggest areas for enhancement.\n\n")

            # æŠ€æœ¯å»ºè®®
            f.write("### Technical Recommendations\n")
            f.write("1. **Production Deployment:** ")
            if 'HD-DDPG' in algorithms:
                hd_ddpg_success = results_df[results_df['algorithm'] == 'HD-DDPG']['success_rate'].mean()
                if hd_ddpg_success > 0.9:
                    f.write("Ready for production deployment with high confidence.\n")
                elif hd_ddpg_success > 0.8:
                    f.write("Suitable for production with proper monitoring.\n")
                else:
                    f.write("Requires further optimization before production deployment.\n")

            f.write("2. **Monitoring:** Implement real-time performance monitoring focusing on makespan stability and quality scores.\n")
            f.write("3. **Optimization:** Consider fine-tuning hyperparameters for specific workflow types showing suboptimal performance.\n")
            f.write("4. **Scalability:** Test with larger workflow sizes and more diverse medical workflow types.\n\n")

            # é™„å½•ä¿¡æ¯
            f.write("## Appendix\n")
            f.write("-" * 10 + "\n\n")
            f.write(f"**Evaluation Configuration:**\n")
            f.write(f"- Test workflows: {args.test_workflows}\n")
            f.write(f"- Workflow sizes: {args.workflow_sizes}\n")
            f.write(f"- Workflow types: {args.workflow_types}\n")
            f.write(f"- Baseline algorithms: {args.baseline_algorithms}\n")
            f.write(f"- Environment failure rate: {args.failure_rate}\n")
            f.write(f"- Random seed: {args.seed}\n\n")

        print(f"âœ… å¢å¼ºè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")


def save_enhanced_results(results_df, output_dir, args):
    """ğŸ¯ ä¿å­˜å¢å¼ºçš„ç»“æœæ•°æ®"""
    print("ğŸ’¾ ä¿å­˜å¢å¼ºç»“æœæ•°æ®...")

    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        if args.export_format in ['csv', 'both']:
            # ä¿å­˜è¯¦ç»†CSV
            csv_path = os.path.join(output_dir, 'enhanced_evaluation_results.csv')
            results_df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"âœ… CSVç»“æœå·²ä¿å­˜: {csv_path}")

            # ä¿å­˜æ±‡æ€»CSV
            summary_df = results_df.groupby(['algorithm', 'workflow_type', 'workflow_size']).agg({
                'makespan': ['mean', 'std', 'count'],
                'quality_score': ['mean', 'std'],
                'stability_score': ['mean', 'std'],
                'success_rate': ['mean', 'std']
            }).round(4)

            summary_csv_path = os.path.join(output_dir, 'performance_summary.csv')
            summary_df.to_csv(summary_csv_path, encoding='utf-8')
            print(f"âœ… æ±‡æ€»CSVå·²ä¿å­˜: {summary_csv_path}")

        if args.export_format in ['json', 'both']:
            # ä¿å­˜JSONæ ¼å¼
            # è½¬æ¢æ•°æ®ä¸ºJSONå…¼å®¹æ ¼å¼
            results_dict = results_df.to_dict('records')

            # å¤„ç†æ— ç©·å€¼
            for record in results_dict:
                for key, value in record.items():
                    if value == float('inf'):
                        record[key] = 'Infinity'
                    elif value == float('-inf'):
                        record[key] = '-Infinity'
                    elif pd.isna(value):
                        record[key] = None

            json_path = os.path.join(output_dir, 'enhanced_evaluation_results.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'metadata': {
                        'evaluation_date': datetime.now().isoformat(),
                        'total_workflows': len(results_df),
                        'algorithms': list(results_df['algorithm'].unique()),
                        'workflow_types': list(results_df['workflow_type'].unique()),
                        'workflow_sizes': list(results_df['workflow_size'].unique()),
                        'evaluation_rounds': args.evaluation_rounds
                    },
                    'results': results_dict
                }, f, indent=2, ensure_ascii=False)

            print(f"âœ… JSONç»“æœå·²ä¿å­˜: {json_path}")

        # ä¿å­˜é…ç½®ä¿¡æ¯
        config_info = {
            'evaluation_args': vars(args),
            'evaluation_timestamp': datetime.now().isoformat(),
            'python_version': sys.version,
            'dependencies': {
                'tqdm_available': TQDM_AVAILABLE,
                'scipy_available': SCIPY_AVAILABLE,
                'stabilized_modules': STABILIZED_MODULES
            }
        }

        config_path = os.path.join(output_dir, 'evaluation_config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, default=str)

        print(f"âœ… é…ç½®ä¿¡æ¯å·²ä¿å­˜: {config_path}")

    except Exception as e:
        print(f"âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")


def main():
    """ğŸ¯ ä¸»è¯„ä¼°å‡½æ•°"""
    print("ğŸ” å¯åŠ¨HD-DDPGå¢å¼ºæ¨¡å‹è¯„ä¼°")
    print("=" * 60)

    # è§£æå‚æ•°
    args = parse_enhanced_arguments()

    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ”§ åŠ è½½æ¨¡å‹...")
    hd_ddpg, config = load_enhanced_model_and_config(args.model_path, args.config_path, args.model_type)
    if hd_ddpg is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    # è§£æè¯„ä¼°å‚æ•°
    workflow_sizes = [int(x.strip()) for x in args.workflow_sizes.split(',')]
    workflow_types = [x.strip() for x in args.workflow_types.split(',')]
    baseline_algorithms = [x.strip() for x in args.baseline_algorithms.split(',')]

    print(f"\nğŸ¯ è¯„ä¼°é…ç½®:")
    print(f"  æµ‹è¯•å·¥ä½œæµ: {args.test_workflows}")
    print(f"  å·¥ä½œæµå¤§å°: {workflow_sizes}")
    print(f"  å·¥ä½œæµç±»å‹: {workflow_types}")
    print(f"  åŸºçº¿ç®—æ³•: {baseline_algorithms}")
    print(f"  è¯„ä¼°è½®æ•°: {args.evaluation_rounds}")
    print(f"  ç½®ä¿¡æ°´å¹³: {args.confidence_level}")
    print("-" * 60)

    # åˆ›å»ºç»„ä»¶
    print("ğŸ—ï¸ åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    environment = create_enhanced_environment(args)

    # åˆ›å»ºå·¥ä½œæµç”Ÿæˆå™¨
    try:
        workflow_generator = MedicalWorkflowGenerator()
        print("âœ… å·¥ä½œæµç”Ÿæˆå™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ å·¥ä½œæµç”Ÿæˆå™¨åˆ›å»ºå¤±è´¥: {e}")
        workflow_generator = None

    # ç”Ÿæˆæµ‹è¯•å·¥ä½œæµé…ç½®
    workflows_per_config = max(1, args.test_workflows // (len(workflow_sizes) * len(workflow_types) * args.evaluation_rounds))
    test_configs = []

    for size in workflow_sizes:
        for wf_type in workflow_types:
            test_configs.append({
                'size': size,
                'type': wf_type,
                'count': workflows_per_config
            })

    # ç”Ÿæˆæµ‹è¯•å·¥ä½œæµ
    print(f"\nğŸ“‹ ç”Ÿæˆæµ‹è¯•å·¥ä½œæµ...")
    test_workflows = generate_enhanced_test_workflows(workflow_generator, test_configs, args)
    print(f"âœ… ç”Ÿæˆäº† {len(test_workflows)} ä¸ªæµ‹è¯•å·¥ä½œæµ")

    # ğŸ¯ å¼€å§‹è¯„ä¼°
    print(f"\nğŸš€ å¼€å§‹å¢å¼ºè¯„ä¼°...")
    evaluation_start_time = time.time()

    # è¯„ä¼°HD-DDPG
    print("\nğŸ¤– è¯„ä¼°HD-DDPG...")
    hd_ddpg_results = evaluate_hd_ddpg_enhanced(hd_ddpg, test_workflows, environment, args)

    # è¯„ä¼°åŸºçº¿ç®—æ³•
    all_results = hd_ddpg_results.copy()

    for baseline in baseline_algorithms:
        print(f"\nğŸ“Š è¯„ä¼°åŸºçº¿ç®—æ³•: {baseline}...")
        baseline_results = evaluate_baseline_enhanced(baseline, test_workflows, environment, args)
        all_results.extend(baseline_results)

    evaluation_duration = time.time() - evaluation_start_time
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼æ€»è€—æ—¶: {evaluation_duration:.2f}ç§’")

    # åˆ›å»ºç»“æœDataFrame
    print("\nğŸ“Š å¤„ç†è¯„ä¼°ç»“æœ...")
    results_df = pd.DataFrame(all_results)

    # æ•°æ®æ¸…ç†
    results_df = results_df.replace([float('inf'), float('-inf')], np.nan)

    # ä¿å­˜åŸå§‹ç»“æœ
    if args.save_detailed_results:
        save_enhanced_results(results_df, args.output_dir, args)

    # æ‰§è¡Œå¢å¼ºç»Ÿè®¡åˆ†æ
    statistical_results = {}
    if args.include_statistical_test:
        print("\nğŸ“ˆ æ‰§è¡Œå¢å¼ºç»Ÿè®¡åˆ†æ...")
        statistical_results = perform_enhanced_statistical_analysis(results_df, args)

    # åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨
    if args.save_plots:
        print("\nğŸ¨ åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨...")
        create_enhanced_evaluation_plots(results_df, args.output_dir, args)

    # ç”Ÿæˆå¢å¼ºè¯„ä¼°æŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆå¢å¼ºè¯„ä¼°æŠ¥å‘Š...")
    generate_enhanced_evaluation_report(results_df, statistical_results, args.output_dir, args)

    # ğŸ¯ æ‰“å°æœ€ç»ˆæ‘˜è¦
    print(f"\nğŸ‰ å¢å¼ºè¯„ä¼°å®Œæˆï¼")
    print("=" * 60)

    # HD-DDPGæ€§èƒ½æ‘˜è¦
    hd_ddpg_data = results_df[results_df['algorithm'] == 'HD-DDPG']
    if len(hd_ddpg_data) > 0:
        print(f"ğŸ“Š HD-DDPGæ€§èƒ½æ‘˜è¦:")

        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_makespan = hd_ddpg_data['makespan'][~hd_ddpg_data['makespan'].isna()]
        if len(valid_makespan) > 0:
            print(f"  å¹³å‡Makespan: {valid_makespan.mean():.3f} Â± {valid_makespan.std():.3f}")

        print(f"  å¹³å‡è´Ÿè½½å‡è¡¡: {hd_ddpg_data['load_balance'].mean():.3f}")
        print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {hd_ddpg_data['quality_score'].mean():.3f}")
        print(f"  å¹³å‡ç¨³å®šæ€§è¯„åˆ†: {hd_ddpg_data['stability_score'].mean():.3f}")
        print(f"  å¹³å‡æˆåŠŸç‡: {hd_ddpg_data['success_rate'].mean():.1%}")
        print(f"  å¹³å‡è°ƒåº¦æ—¶é—´: {hd_ddpg_data['scheduling_time'].mean():.6f}ç§’")

    # æ¯”è¾ƒç»“æœ
    if len(baseline_algorithms) > 0:
        print(f"\nğŸ“ˆ ä¸åŸºçº¿ç®—æ³•æ¯”è¾ƒ:")
        for baseline in baseline_algorithms:
            baseline_data = results_df[results_df['algorithm'] == baseline]
            if len(baseline_data) > 0 and len(hd_ddpg_data) > 0:
                hd_makespan = hd_ddpg_data['makespan'][~hd_ddpg_data['makespan'].isna()].mean()
                base_makespan = baseline_data['makespan'][~baseline_data['makespan'].isna()].mean()

                if not pd.isna(hd_makespan) and not pd.isna(base_makespan) and base_makespan > 0:
                    improvement = (base_makespan - hd_makespan) / base_makespan * 100
                    print(f"  vs {baseline}: {improvement:+.1f}% makespanæ”¹è¿›")

    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: enhanced_evaluation_report.md")
    print(f"ğŸ“Š å›¾è¡¨æ–‡ä»¶: *.png")
    print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: enhanced_evaluation_results.{args.export_format}")

    print("\n" + "=" * 60)
    print("ğŸ HD-DDPGå¢å¼ºè¯„ä¼°ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()