"""
Performance Metrics for Medical Workflow Scheduling - é«˜åº¦å¢å¼ºç‰ˆæœ¬
åŒ»ç–—å·¥ä½œæµè°ƒåº¦æ€§èƒ½æŒ‡æ ‡ - ä¸“ä¸ºç¨³å®šåŒ–HD-DDPGè®¾è®¡

ğŸ¯ ä¸»è¦å¢å¼ºï¼š
- ç¨³å®šæ€§æŒ‡æ ‡ç›‘æ§
- å®æ—¶æ€§èƒ½è·Ÿè¸ª
- æ”¶æ•›æ€§åˆ†æ
- å¼‚å¸¸æ£€æµ‹
- é«˜çº§å¯è§†åŒ–
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import deque
from typing import Dict, List, Tuple, Optional
import pandas as pd
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")


class EnhancedSchedulingMetrics:
    """
    å¢å¼ºçš„è°ƒåº¦æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨
    ğŸ¯ ä¸“ä¸ºç¨³å®šåŒ–è®­ç»ƒå’Œæ€§èƒ½åˆ†æè®¾è®¡
    """

    def __init__(self, window_size: int = 100, stability_threshold: float = 0.1):
        """
        åˆå§‹åŒ–å¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡ç³»ç»Ÿ

        Args:
            window_size: æ€§èƒ½çª—å£å¤§å°
            stability_threshold: ç¨³å®šæ€§é˜ˆå€¼
        """
        self.window_size = window_size
        self.stability_threshold = stability_threshold

        print("ğŸ”§ åˆå§‹åŒ–EnhancedSchedulingMetrics...")

        # ğŸ¯ åŸºç¡€æ€§èƒ½æŒ‡æ ‡
        self.basic_metrics = {
            'makespans': deque(maxlen=1000),
            'load_balances': deque(maxlen=1000),
            'energy_consumptions': deque(maxlen=1000),
            'throughputs': deque(maxlen=1000),
            'episode_rewards': deque(maxlen=1000),
            'success_rates': deque(maxlen=1000)
        }

        # ğŸ¯ ç¨³å®šæ€§æŒ‡æ ‡
        self.stability_metrics = {
            'makespan_stability': deque(maxlen=500),
            'reward_variance': deque(maxlen=500),
            'performance_consistency': deque(maxlen=500),
            'convergence_score': deque(maxlen=500),
            'volatility_index': deque(maxlen=500)
        }

        # ğŸ¯ å®æ—¶ç›‘æ§æŒ‡æ ‡
        self.realtime_metrics = {
            'running_averages': deque(maxlen=100),
            'trend_indicators': deque(maxlen=100),
            'anomaly_scores': deque(maxlen=100),
            'improvement_rates': deque(maxlen=100)
        }

        # ğŸ¯ æ”¶æ•›æ€§åˆ†æ
        self.convergence_analysis = {
            'plateau_detection': deque(maxlen=50),
            'learning_rate_estimates': deque(maxlen=50),
            'convergence_timestamps': [],
            'best_performance_episodes': []
        }

        # ğŸ¯ æ¯”è¾ƒåŸºå‡†
        self.baseline_comparisons = {
            'heft_comparison': deque(maxlen=100),
            'fcfs_comparison': deque(maxlen=100),
            'random_comparison': deque(maxlen=100),
            'improvement_ratios': deque(maxlen=100)
        }

        # ğŸ¯ ç»Ÿè®¡æ±‡æ€»
        self.statistics_summary = {
            'best_makespan': float('inf'),
            'best_reward': float('-inf'),
            'worst_makespan': 0,
            'worst_reward': float('inf'),
            'total_episodes': 0,
            'stable_episodes': 0,
            'convergence_episode': None
        }

        print("âœ… å¢å¼ºæ€§èƒ½æŒ‡æ ‡ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def reset(self):
        """ğŸ¯ æ™ºèƒ½é‡ç½® - ä¿ç•™é‡è¦å†å²ä¿¡æ¯"""
        try:
            # ä¿å­˜å…³é”®ç»Ÿè®¡ä¿¡æ¯
            if self.basic_metrics['makespans']:
                self.statistics_summary['total_episodes'] += len(self.basic_metrics['makespans'])

            # æ¸…ç©ºå®æ—¶æŒ‡æ ‡
            for metric_dict in [self.basic_metrics, self.stability_metrics,
                               self.realtime_metrics, self.convergence_analysis]:
                for key, deque_obj in metric_dict.items():
                    if isinstance(deque_obj, deque):
                        deque_obj.clear()

            # é‡ç½®éƒ¨åˆ†ç»Ÿè®¡æ‘˜è¦ï¼ˆä¿ç•™æœ€ä½³è®°å½•ï¼‰
            self.statistics_summary.update({
                'stable_episodes': 0,
                'convergence_episode': None
            })

            print("ğŸ”„ æ€§èƒ½æŒ‡æ ‡å·²æ™ºèƒ½é‡ç½®")

        except Exception as e:
            print(f"âš ï¸ æŒ‡æ ‡é‡ç½®é”™è¯¯: {e}")

    def calculate_makespan(self, task_completion_times: List[float]) -> float:
        """ğŸ¯ å¢å¼ºçš„å®Œå·¥æ—¶é—´è®¡ç®—"""
        try:
            if not task_completion_times:
                return float('inf')

            # è¿‡æ»¤æ— æ•ˆå€¼
            valid_times = [t for t in task_completion_times if t != float('inf') and not np.isnan(t)]

            if not valid_times:
                return float('inf')

            makespan = max(valid_times)

            # ğŸ¯ å¼‚å¸¸å€¼æ£€æµ‹
            if makespan > 1000:  # å¼‚å¸¸å¤§çš„makespan
                print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸makespan: {makespan}")

            return makespan

        except Exception as e:
            print(f"âš ï¸ Makespanè®¡ç®—é”™è¯¯: {e}")
            return float('inf')

    def calculate_enhanced_load_balance(self, node_loads: List[float]) -> float:
        """ğŸ¯ å¢å¼ºçš„è´Ÿè½½å‡è¡¡è®¡ç®—"""
        try:
            if not node_loads:
                return 0.0

            # è¿‡æ»¤æ— æ•ˆè´Ÿè½½
            valid_loads = [load for load in node_loads if not np.isnan(load) and load >= 0]

            if not valid_loads:
                return 0.0

            if len(valid_loads) == 1:
                return 1.0

            mean_load = np.mean(valid_loads)

            if mean_load == 0:
                return 1.0

            # ğŸ¯ æ”¹è¿›çš„è´Ÿè½½å‡è¡¡è®¡ç®—
            std_load = np.std(valid_loads)
            cv = std_load / mean_load  # å˜å¼‚ç³»æ•°

            # ä½¿ç”¨æ›´å¹³æ»‘çš„å‡½æ•°
            load_balance = 1.0 / (1.0 + cv)

            # ğŸ¯ é¢å¤–çš„å‡åŒ€æ€§æ£€æŸ¥
            max_load = max(valid_loads)
            min_load = min(valid_loads)

            if max_load > 0:
                uniformity = 1.0 - (max_load - min_load) / max_load
                load_balance = (load_balance + uniformity) / 2

            return min(1.0, max(0.0, load_balance))

        except Exception as e:
            print(f"âš ï¸ è´Ÿè½½å‡è¡¡è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def calculate_stability_score(self, metric_history: List[float], window: int = 20) -> float:
        """ğŸ¯ è®¡ç®—æŒ‡æ ‡ç¨³å®šæ€§è¯„åˆ†"""
        try:
            if len(metric_history) < window:
                return 0.0

            recent_values = list(metric_history)[-window:]

            # è¿‡æ»¤æ— æ•ˆå€¼
            valid_values = [v for v in recent_values if not np.isnan(v) and v != float('inf')]

            if len(valid_values) < 3:
                return 0.0

            # è®¡ç®—å˜å¼‚ç³»æ•°
            mean_val = np.mean(valid_values)
            std_val = np.std(valid_values)

            if mean_val == 0:
                return 1.0 if std_val == 0 else 0.0

            cv = std_val / abs(mean_val)

            # ç¨³å®šæ€§è¯„åˆ† (å˜å¼‚ç³»æ•°è¶Šå°è¶Šç¨³å®š)
            stability = 1.0 / (1.0 + cv * 5)

            return min(1.0, max(0.0, stability))

        except Exception as e:
            print(f"âš ï¸ ç¨³å®šæ€§è¯„åˆ†è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def calculate_convergence_score(self, metric_history: List[float], window: int = 50) -> float:
        """ğŸ¯ è®¡ç®—æ”¶æ•›æ€§è¯„åˆ†"""
        try:
            if len(metric_history) < window:
                return 0.0

            recent_values = list(metric_history)[-window:]
            valid_values = [v for v in recent_values if not np.isnan(v) and v != float('inf')]

            if len(valid_values) < 10:
                return 0.0

            # è®¡ç®—è¶‹åŠ¿æ–œç‡
            x = np.arange(len(valid_values))
            slope, _ = np.polyfit(x, valid_values, 1)

            # è®¡ç®—æ–¹å·®
            variance = np.var(valid_values)
            mean_val = np.mean(valid_values)

            # æ”¶æ•›è¯„åˆ†ç»¼åˆè€ƒè™‘è¶‹åŠ¿å¹³ç¼“ç¨‹åº¦å’Œæ–¹å·®
            if mean_val != 0:
                trend_score = 1.0 / (1.0 + abs(slope) * 100)
                variance_score = 1.0 / (1.0 + variance / abs(mean_val))
                convergence = (trend_score + variance_score) / 2
            else:
                convergence = 0.5

            return min(1.0, max(0.0, convergence))

        except Exception as e:
            print(f"âš ï¸ æ”¶æ•›æ€§è¯„åˆ†è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def calculate_anomaly_score(self, current_value: float, history: List[float]) -> float:
        """ğŸ¯ è®¡ç®—å¼‚å¸¸æ£€æµ‹è¯„åˆ†"""
        try:
            if len(history) < 10:
                return 0.0

            valid_history = [v for v in history if not np.isnan(v) and v != float('inf')]

            if len(valid_history) < 5:
                return 0.0

            mean_val = np.mean(valid_history)
            std_val = np.std(valid_history)

            if std_val == 0:
                return 0.0 if current_value == mean_val else 1.0

            # Z-scoreå¼‚å¸¸æ£€æµ‹
            z_score = abs(current_value - mean_val) / std_val
            anomaly_score = min(1.0, z_score / 3.0)  # 3-sigmaåŸåˆ™

            return anomaly_score

        except Exception as e:
            print(f"âš ï¸ å¼‚å¸¸æ£€æµ‹è¯„åˆ†è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def update_metrics(self, makespan: float, load_balance: float, energy: float,
                      throughput: float, reward: float, success_rate: float = 1.0,
                      episode: int = None):
        """ğŸ¯ æ›´æ–°æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡"""
        try:
            # ğŸ¯ æ›´æ–°åŸºç¡€æŒ‡æ ‡
            self.basic_metrics['makespans'].append(makespan)
            self.basic_metrics['load_balances'].append(load_balance)
            self.basic_metrics['energy_consumptions'].append(energy)
            self.basic_metrics['throughputs'].append(throughput)
            self.basic_metrics['episode_rewards'].append(reward)
            self.basic_metrics['success_rates'].append(success_rate)

            # ğŸ¯ è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
            makespan_stability = self.calculate_stability_score(list(self.basic_metrics['makespans']))
            reward_variance = np.var(list(self.basic_metrics['episode_rewards'])[-20:]) if len(self.basic_metrics['episode_rewards']) >= 20 else 0
            convergence_score = self.calculate_convergence_score(list(self.basic_metrics['makespans']))

            self.stability_metrics['makespan_stability'].append(makespan_stability)
            self.stability_metrics['reward_variance'].append(reward_variance)
            self.stability_metrics['convergence_score'].append(convergence_score)

            # ğŸ¯ è®¡ç®—å®æ—¶æŒ‡æ ‡
            running_avg = np.mean(list(self.basic_metrics['makespans'])[-self.window_size:])
            anomaly_score = self.calculate_anomaly_score(makespan, list(self.basic_metrics['makespans']))

            self.realtime_metrics['running_averages'].append(running_avg)
            self.realtime_metrics['anomaly_scores'].append(anomaly_score)

            # ğŸ¯ æ›´æ–°ç»Ÿè®¡æ‘˜è¦
            self._update_statistics_summary(makespan, reward, makespan_stability, episode)

            # ğŸ¯ æ£€æµ‹æ”¶æ•›
            self._detect_convergence(convergence_score, episode)

        except Exception as e:
            print(f"âš ï¸ æŒ‡æ ‡æ›´æ–°é”™è¯¯: {e}")

    def _update_statistics_summary(self, makespan: float, reward: float,
                                 stability: float, episode: int):
        """ğŸ¯ æ›´æ–°ç»Ÿè®¡æ‘˜è¦"""
        try:
            # æ›´æ–°æœ€ä½³/æœ€å·®è®°å½•
            if makespan != float('inf') and makespan < self.statistics_summary['best_makespan']:
                self.statistics_summary['best_makespan'] = makespan
                if episode is not None:
                    self.convergence_analysis['best_performance_episodes'].append(episode)

            if reward > self.statistics_summary['best_reward']:
                self.statistics_summary['best_reward'] = reward

            if makespan != float('inf'):
                self.statistics_summary['worst_makespan'] = max(self.statistics_summary['worst_makespan'], makespan)

            self.statistics_summary['worst_reward'] = min(self.statistics_summary['worst_reward'], reward)

            # ç¨³å®šæ€§ç»Ÿè®¡
            if stability > 0.8:
                self.statistics_summary['stable_episodes'] += 1

        except Exception as e:
            print(f"âš ï¸ ç»Ÿè®¡æ‘˜è¦æ›´æ–°é”™è¯¯: {e}")

    def _detect_convergence(self, convergence_score: float, episode: int):
        """ğŸ¯ æ£€æµ‹è®­ç»ƒæ”¶æ•›"""
        try:
            if convergence_score > 0.9 and episode is not None:
                if self.statistics_summary['convergence_episode'] is None:
                    self.statistics_summary['convergence_episode'] = episode
                    self.convergence_analysis['convergence_timestamps'].append({
                        'episode': episode,
                        'timestamp': datetime.now().isoformat(),
                        'score': convergence_score
                    })
                    print(f"ğŸ‰ æ£€æµ‹åˆ°è®­ç»ƒæ”¶æ•› (Episode {episode}, Score: {convergence_score:.3f})")

        except Exception as e:
            print(f"âš ï¸ æ”¶æ•›æ£€æµ‹é”™è¯¯: {e}")

    def add_baseline_comparison(self, algorithm: str, makespan: float):
        """ğŸ¯ æ·»åŠ åŸºçº¿ç®—æ³•æ¯”è¾ƒ"""
        try:
            if algorithm.upper() in ['HEFT', 'FCFS', 'RANDOM']:
                metric_key = f'{algorithm.lower()}_comparison'
                if metric_key in self.baseline_comparisons:
                    self.baseline_comparisons[metric_key].append(makespan)

                    # è®¡ç®—æ”¹è¿›æ¯”ä¾‹
                    if self.basic_metrics['makespans'] and makespan > 0:
                        current_makespan = list(self.basic_metrics['makespans'])[-1]
                        if current_makespan != float('inf'):
                            improvement = (makespan - current_makespan) / makespan * 100
                            self.baseline_comparisons['improvement_ratios'].append(improvement)

        except Exception as e:
            print(f"âš ï¸ åŸºçº¿æ¯”è¾ƒæ·»åŠ é”™è¯¯: {e}")

    def get_comprehensive_metrics(self, last_n: int = 100) -> Dict:
        """ğŸ¯ è·å–ç»¼åˆæ€§èƒ½æŒ‡æ ‡"""
        try:
            def safe_avg(lst, default=0):
                valid_values = [v for v in list(lst)[-last_n:] if not np.isnan(v) and v != float('inf')]
                return np.mean(valid_values) if valid_values else default

            def safe_std(lst, default=0):
                valid_values = [v for v in list(lst)[-last_n:] if not np.isnan(v) and v != float('inf')]
                return np.std(valid_values) if len(valid_values) > 1 else default

            comprehensive_metrics = {
                # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
                'performance': {
                    'avg_makespan': safe_avg(self.basic_metrics['makespans']),
                    'std_makespan': safe_std(self.basic_metrics['makespans']),
                    'avg_reward': safe_avg(self.basic_metrics['episode_rewards']),
                    'std_reward': safe_std(self.basic_metrics['episode_rewards']),
                    'avg_load_balance': safe_avg(self.basic_metrics['load_balances']),
                    'avg_energy': safe_avg(self.basic_metrics['energy_consumptions']),
                    'avg_throughput': safe_avg(self.basic_metrics['throughputs']),
                    'avg_success_rate': safe_avg(self.basic_metrics['success_rates'])
                },

                # ç¨³å®šæ€§æŒ‡æ ‡
                'stability': {
                    'makespan_stability': safe_avg(self.stability_metrics['makespan_stability']),
                    'reward_variance': safe_avg(self.stability_metrics['reward_variance']),
                    'convergence_score': safe_avg(self.stability_metrics['convergence_score']),
                    'anomaly_rate': len([s for s in list(self.realtime_metrics['anomaly_scores'])[-last_n:] if s > 0.5]) / min(last_n, len(self.realtime_metrics['anomaly_scores'])) if self.realtime_metrics['anomaly_scores'] else 0
                },

                # æ”¶æ•›æ€§åˆ†æ
                'convergence': {
                    'converged': self.statistics_summary['convergence_episode'] is not None,
                    'convergence_episode': self.statistics_summary['convergence_episode'],
                    'stable_episode_ratio': self.statistics_summary['stable_episodes'] / max(1, len(self.basic_metrics['makespans'])),
                    'improvement_trend': self._calculate_improvement_trend()
                },

                # ç»Ÿè®¡æ‘˜è¦
                'summary': {
                    'best_makespan': self.statistics_summary['best_makespan'],
                    'best_reward': self.statistics_summary['best_reward'],
                    'total_episodes': len(self.basic_metrics['makespans']),
                    'data_quality': self._assess_data_quality()
                }
            }

            # åŸºçº¿æ¯”è¾ƒ (å¦‚æœæœ‰æ•°æ®)
            if any(self.baseline_comparisons[key] for key in ['heft_comparison', 'fcfs_comparison', 'random_comparison']):
                comprehensive_metrics['baseline_comparison'] = {
                    'avg_improvement': safe_avg(self.baseline_comparisons['improvement_ratios']),
                    'heft_comparison': safe_avg(self.baseline_comparisons['heft_comparison']) if self.baseline_comparisons['heft_comparison'] else None,
                    'fcfs_comparison': safe_avg(self.baseline_comparisons['fcfs_comparison']) if self.baseline_comparisons['fcfs_comparison'] else None,
                    'random_comparison': safe_avg(self.baseline_comparisons['random_comparison']) if self.baseline_comparisons['random_comparison'] else None
                }

            return comprehensive_metrics

        except Exception as e:
            print(f"âš ï¸ ç»¼åˆæŒ‡æ ‡è®¡ç®—é”™è¯¯: {e}")
            return {'error': str(e)}

    def _calculate_improvement_trend(self) -> str:
        """ğŸ¯ è®¡ç®—æ”¹è¿›è¶‹åŠ¿"""
        try:
            if len(self.basic_metrics['makespans']) < 20:
                return 'insufficient_data'

            recent_makespans = [m for m in list(self.basic_metrics['makespans'])[-20:]
                              if m != float('inf') and not np.isnan(m)]

            if len(recent_makespans) < 10:
                return 'insufficient_valid_data'

            # è®¡ç®—çº¿æ€§è¶‹åŠ¿
            x = np.arange(len(recent_makespans))
            slope, _ = np.polyfit(x, recent_makespans, 1)

            if slope < -0.01:
                return 'improving'
            elif slope > 0.01:
                return 'degrading'
            else:
                return 'stable'

        except Exception as e:
            print(f"âš ï¸ æ”¹è¿›è¶‹åŠ¿è®¡ç®—é”™è¯¯: {e}")
            return 'unknown'

    def _assess_data_quality(self) -> float:
        """ğŸ¯ è¯„ä¼°æ•°æ®è´¨é‡"""
        try:
            total_points = len(self.basic_metrics['makespans'])
            if total_points == 0:
                return 0.0

            # æ£€æŸ¥æ— æ•ˆæ•°æ®ç‚¹
            invalid_makespans = len([m for m in self.basic_metrics['makespans']
                                   if m == float('inf') or np.isnan(m)])
            invalid_rewards = len([r for r in self.basic_metrics['episode_rewards']
                                 if np.isnan(r)])

            total_invalid = invalid_makespans + invalid_rewards
            quality_score = 1.0 - (total_invalid / (total_points * 2))

            return max(0.0, min(1.0, quality_score))

        except Exception as e:
            print(f"âš ï¸ æ•°æ®è´¨é‡è¯„ä¼°é”™è¯¯: {e}")
            return 0.5

    def create_comprehensive_visualization(self, save_path: str = None, show_plot: bool = True):
        """ğŸ¯ åˆ›å»ºç»¼åˆæ€§èƒ½å¯è§†åŒ–"""
        try:
            # åˆ›å»ºå¤§å›¾è¡¨
            fig = plt.figure(figsize=(20, 16))
            gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)

            # 1. åŸºç¡€æ€§èƒ½è¶‹åŠ¿ (2x2)
            ax1 = fig.add_subplot(gs[0, :2])
            self._plot_performance_trends(ax1)

            # 2. ç¨³å®šæ€§åˆ†æ (2x2)
            ax2 = fig.add_subplot(gs[0, 2:])
            self._plot_stability_analysis(ax2)

            # 3. æ”¶æ•›æ€§åˆ†æ
            ax3 = fig.add_subplot(gs[1, :2])
            self._plot_convergence_analysis(ax3)

            # 4. å¼‚å¸¸æ£€æµ‹
            ax4 = fig.add_subplot(gs[1, 2:])
            self._plot_anomaly_detection(ax4)

            # 5. åˆ†å¸ƒåˆ†æ
            ax5 = fig.add_subplot(gs[2, :2])
            self._plot_distribution_analysis(ax5)

            # 6. åŸºçº¿æ¯”è¾ƒ (å¦‚æœæœ‰æ•°æ®)
            ax6 = fig.add_subplot(gs[2, 2:])
            self._plot_baseline_comparison(ax6)

            # 7. ç›¸å…³æ€§åˆ†æ
            ax7 = fig.add_subplot(gs[3, :2])
            self._plot_correlation_analysis(ax7)

            # 8. æ€§èƒ½æ‘˜è¦
            ax8 = fig.add_subplot(gs[3, 2:])
            self._plot_performance_summary(ax8)

            plt.suptitle('HD-DDPG Comprehensive Performance Analysis', fontsize=16, fontweight='bold')

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"âœ… ç»¼åˆå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")

            if show_plot:
                plt.show()
            else:
                plt.close()

        except Exception as e:
            print(f"âš ï¸ ç»¼åˆå¯è§†åŒ–åˆ›å»ºé”™è¯¯: {e}")
            plt.close()

    def _plot_performance_trends(self, ax):
        """ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿"""
        try:
            if not self.basic_metrics['makespans']:
                ax.text(0.5, 0.5, 'No Data Available', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Performance Trends')
                return

            episodes = range(len(self.basic_metrics['makespans']))

            # åŒyè½´
            ax2 = ax.twinx()

            # Makespan
            valid_makespans = [m if m != float('inf') else np.nan for m in self.basic_metrics['makespans']]
            line1 = ax.plot(episodes, valid_makespans, 'b-', alpha=0.7, label='Makespan')

            # ç§»åŠ¨å¹³å‡
            if len(valid_makespans) >= 10:
                ma = pd.Series(valid_makespans).rolling(window=10, min_periods=1).mean()
                ax.plot(episodes, ma, 'b-', linewidth=2, label='Makespan MA(10)')

            # å¥–åŠ±
            line2 = ax2.plot(episodes, list(self.basic_metrics['episode_rewards']), 'r-', alpha=0.7, label='Reward')

            ax.set_xlabel('Episode')
            ax.set_ylabel('Makespan', color='b')
            ax2.set_ylabel('Reward', color='r')
            ax.set_title('Performance Trends')
            ax.grid(True, alpha=0.3)

            # åˆå¹¶å›¾ä¾‹
            lines1, labels1 = ax.get_legend_handles_labels()
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

        except Exception as e:
            print(f"âš ï¸ æ€§èƒ½è¶‹åŠ¿ç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_stability_analysis(self, ax):
        """ç»˜åˆ¶ç¨³å®šæ€§åˆ†æ"""
        try:
            if not self.stability_metrics['makespan_stability']:
                ax.text(0.5, 0.5, 'No Stability Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Stability Analysis')
                return

            episodes = range(len(self.stability_metrics['makespan_stability']))

            ax.plot(episodes, list(self.stability_metrics['makespan_stability']),
                   'g-', linewidth=2, label='Makespan Stability')
            ax.plot(episodes, list(self.stability_metrics['convergence_score']),
                   'orange', linewidth=2, label='Convergence Score')

            # ç¨³å®šæ€§é˜ˆå€¼çº¿
            ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Stability Threshold (0.8)')

            ax.set_xlabel('Episode')
            ax.set_ylabel('Stability Score')
            ax.set_title('Stability Analysis')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1)

        except Exception as e:
            print(f"âš ï¸ ç¨³å®šæ€§åˆ†æç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_convergence_analysis(self, ax):
        """ç»˜åˆ¶æ”¶æ•›æ€§åˆ†æ"""
        try:
            if len(self.basic_metrics['makespans']) < 20:
                ax.text(0.5, 0.5, 'Insufficient Data for Convergence Analysis',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Convergence Analysis')
                return

            # è®¡ç®—ç´¯ç§¯æœ€ä¼˜
            makespans = [m if m != float('inf') else np.nan for m in self.basic_metrics['makespans']]
            cumulative_best = pd.Series(makespans).expanding().min()

            episodes = range(len(makespans))
            ax.plot(episodes, cumulative_best, 'purple', linewidth=2, label='Cumulative Best Makespan')

            # æ ‡è®°æ”¶æ•›ç‚¹
            if self.statistics_summary['convergence_episode'] is not None:
                conv_ep = self.statistics_summary['convergence_episode']
                if conv_ep < len(cumulative_best):
                    ax.axvline(x=conv_ep, color='red', linestyle='--', alpha=0.8,
                             label=f'Convergence (Ep {conv_ep})')

            ax.set_xlabel('Episode')
            ax.set_ylabel('Best Makespan')
            ax.set_title('Convergence Analysis')
            ax.legend()
            ax.grid(True, alpha=0.3)

        except Exception as e:
            print(f"âš ï¸ æ”¶æ•›åˆ†æç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_anomaly_detection(self, ax):
        """ç»˜åˆ¶å¼‚å¸¸æ£€æµ‹"""
        try:
            if not self.realtime_metrics['anomaly_scores']:
                ax.text(0.5, 0.5, 'No Anomaly Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Anomaly Detection')
                return

            episodes = range(len(self.realtime_metrics['anomaly_scores']))
            anomaly_scores = list(self.realtime_metrics['anomaly_scores'])

            # å¼‚å¸¸è¯„åˆ†
            ax.plot(episodes, anomaly_scores, 'red', alpha=0.7, label='Anomaly Score')

            # å¼‚å¸¸é˜ˆå€¼
            ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Anomaly Threshold (0.5)')

            # é«˜äº®å¼‚å¸¸ç‚¹
            high_anomaly = [i for i, score in enumerate(anomaly_scores) if score > 0.5]
            if high_anomaly:
                ax.scatter([episodes[i] for i in high_anomaly],
                          [anomaly_scores[i] for i in high_anomaly],
                          color='red', s=50, alpha=0.8, label='Anomalies')

            ax.set_xlabel('Episode')
            ax.set_ylabel('Anomaly Score')
            ax.set_title('Anomaly Detection')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1)

        except Exception as e:
            print(f"âš ï¸ å¼‚å¸¸æ£€æµ‹ç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_distribution_analysis(self, ax):
        """ç»˜åˆ¶åˆ†å¸ƒåˆ†æ"""
        try:
            if not self.basic_metrics['makespans']:
                ax.text(0.5, 0.5, 'No Data for Distribution', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Makespan Distribution')
                return

            # è¿‡æ»¤æœ‰æ•ˆmakespan
            valid_makespans = [m for m in self.basic_metrics['makespans']
                             if m != float('inf') and not np.isnan(m)]

            if not valid_makespans:
                ax.text(0.5, 0.5, 'No Valid Makespan Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Makespan Distribution')
                return

            # ç›´æ–¹å›¾
            ax.hist(valid_makespans, bins=30, alpha=0.7, color='skyblue', edgecolor='black')

            # ç»Ÿè®¡çº¿
            mean_val = np.mean(valid_makespans)
            ax.axvline(x=mean_val, color='red', linestyle='-', linewidth=2, label=f'Mean: {mean_val:.3f}')

            median_val = np.median(valid_makespans)
            ax.axvline(x=median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.3f}')

            ax.set_xlabel('Makespan')
            ax.set_ylabel('Frequency')
            ax.set_title('Makespan Distribution')
            ax.legend()
            ax.grid(True, alpha=0.3)

        except Exception as e:
            print(f"âš ï¸ åˆ†å¸ƒåˆ†æç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_baseline_comparison(self, ax):
        """ç»˜åˆ¶åŸºçº¿æ¯”è¾ƒ"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºçº¿æ•°æ®
            has_baseline_data = any(self.baseline_comparisons[key] for key in
                                  ['heft_comparison', 'fcfs_comparison', 'random_comparison'])

            if not has_baseline_data:
                ax.text(0.5, 0.5, 'No Baseline Comparison Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Baseline Comparison')
                return

            # å‡†å¤‡æ•°æ®
            algorithms = []
            avg_makespans = []

            # HD-DDPG
            valid_makespans = [m for m in self.basic_metrics['makespans']
                             if m != float('inf') and not np.isnan(m)]
            if valid_makespans:
                algorithms.append('HD-DDPG')
                avg_makespans.append(np.mean(valid_makespans[-50:]))  # æœ€è¿‘50ä¸ª

            # åŸºçº¿ç®—æ³•
            for algo, key in [('HEFT', 'heft_comparison'), ('FCFS', 'fcfs_comparison'), ('Random', 'random_comparison')]:
                if self.baseline_comparisons[key]:
                    algorithms.append(algo)
                    avg_makespans.append(np.mean(list(self.baseline_comparisons[key])))

            if len(algorithms) > 1:
                bars = ax.bar(algorithms, avg_makespans, color=['blue', 'red', 'green', 'orange'][:len(algorithms)])

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}', ha='center', va='bottom')

                ax.set_ylabel('Average Makespan')
                ax.set_title('Algorithm Comparison')
                ax.grid(True, alpha=0.3)
            else:
                ax.text(0.5, 0.5, 'Insufficient Comparison Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Baseline Comparison')

        except Exception as e:
            print(f"âš ï¸ åŸºçº¿æ¯”è¾ƒç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_correlation_analysis(self, ax):
        """ç»˜åˆ¶ç›¸å…³æ€§åˆ†æ"""
        try:
            if len(self.basic_metrics['makespans']) < 10:
                ax.text(0.5, 0.5, 'Insufficient Data for Correlation', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Correlation Analysis')
                return

            # å‡†å¤‡æ•°æ®
            valid_indices = []
            makespans = []
            rewards = []
            load_balances = []

            for i, (m, r, lb) in enumerate(zip(self.basic_metrics['makespans'],
                                             self.basic_metrics['episode_rewards'],
                                             self.basic_metrics['load_balances'])):
                if m != float('inf') and not np.isnan(m) and not np.isnan(r) and not np.isnan(lb):
                    makespans.append(m)
                    rewards.append(r)
                    load_balances.append(lb)

            if len(makespans) < 5:
                ax.text(0.5, 0.5, 'Insufficient Valid Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Correlation Analysis')
                return

            # Makespan vs Reward æ•£ç‚¹å›¾
            ax.scatter(makespans, rewards, alpha=0.6, color='blue', s=30)

            # æ‹Ÿåˆè¶‹åŠ¿çº¿
            if len(makespans) >= 5:
                z = np.polyfit(makespans, rewards, 1)
                p = np.poly1d(z)
                ax.plot(makespans, p(makespans), "r--", alpha=0.8, linewidth=2)

                # è®¡ç®—ç›¸å…³ç³»æ•°
                correlation = np.corrcoef(makespans, rewards)[0, 1]
                ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}',
                       transform=ax.transAxes, bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))

            ax.set_xlabel('Makespan')
            ax.set_ylabel('Reward')
            ax.set_title('Makespan vs Reward Correlation')
            ax.grid(True, alpha=0.3)

        except Exception as e:
            print(f"âš ï¸ ç›¸å…³æ€§åˆ†æç»˜åˆ¶é”™è¯¯: {e}")

    def _plot_performance_summary(self, ax):
        """ç»˜åˆ¶æ€§èƒ½æ‘˜è¦"""
        try:
            # éšè—åæ ‡è½´
            ax.axis('off')

            # è·å–ç»¼åˆæŒ‡æ ‡
            metrics = self.get_comprehensive_metrics()

            # åˆ›å»ºæ‘˜è¦æ–‡æœ¬
            summary_text = f"""
Performance Summary

ğŸ¯ Current Performance:
  â€¢ Avg Makespan: {metrics['performance']['avg_makespan']:.3f}
  â€¢ Avg Reward: {metrics['performance']['avg_reward']:.2f}
  â€¢ Success Rate: {metrics['performance']['avg_success_rate']:.1%}

ğŸ“Š Stability Metrics:
  â€¢ Makespan Stability: {metrics['stability']['makespan_stability']:.3f}
  â€¢ Convergence Score: {metrics['stability']['convergence_score']:.3f}
  â€¢ Anomaly Rate: {metrics['stability']['anomaly_rate']:.1%}

ğŸ† Best Records:
  â€¢ Best Makespan: {metrics['summary']['best_makespan']:.3f}
  â€¢ Best Reward: {metrics['summary']['best_reward']:.2f}

ğŸ“ˆ Training Status:
  â€¢ Total Episodes: {metrics['summary']['total_episodes']}
  â€¢ Converged: {'Yes' if metrics['convergence']['converged'] else 'No'}
  â€¢ Data Quality: {metrics['summary']['data_quality']:.1%}
            """

            ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=10,
                   verticalalignment='top', fontfamily='monospace',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.8))

            ax.set_title('Performance Summary', fontweight='bold')

        except Exception as e:
            print(f"âš ï¸ æ€§èƒ½æ‘˜è¦ç»˜åˆ¶é”™è¯¯: {e}")

    def export_metrics_to_csv(self, filepath: str):
        """ğŸ¯ å¯¼å‡ºæŒ‡æ ‡åˆ°CSV"""
        try:
            # å‡†å¤‡æ•°æ®
            data = {
                'episode': range(len(self.basic_metrics['makespans'])),
                'makespan': list(self.basic_metrics['makespans']),
                'reward': list(self.basic_metrics['episode_rewards']),
                'load_balance': list(self.basic_metrics['load_balances']),
                'energy': list(self.basic_metrics['energy_consumptions']),
                'throughput': list(self.basic_metrics['throughputs']),
                'success_rate': list(self.basic_metrics['success_rates'])
            }

            # æ·»åŠ ç¨³å®šæ€§æŒ‡æ ‡
            for i, stability in enumerate(self.stability_metrics['makespan_stability']):
                if i < len(data['episode']):
                    if 'makespan_stability' not in data:
                        data['makespan_stability'] = [None] * len(data['episode'])
                    data['makespan_stability'][i] = stability

            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            df.to_csv(filepath, index=False)

            print(f"âœ… æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {filepath}")

        except Exception as e:
            print(f"âš ï¸ æŒ‡æ ‡å¯¼å‡ºé”™è¯¯: {e}")

    def export_summary_to_json(self, filepath: str):
        """ğŸ¯ å¯¼å‡ºæ‘˜è¦åˆ°JSON"""
        try:
            summary = self.get_comprehensive_metrics()
            summary['export_timestamp'] = datetime.now().isoformat()

            with open(filepath, 'w') as f:
                json.dump(summary, f, indent=2, default=str)

            print(f"âœ… æ‘˜è¦å·²å¯¼å‡ºåˆ°: {filepath}")

        except Exception as e:
            print(f"âš ï¸ æ‘˜è¦å¯¼å‡ºé”™è¯¯: {e}")


# ğŸ¯ ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå§‹ç±»å
SchedulingMetrics = EnhancedSchedulingMetrics


# ğŸ§ª å¢å¼ºçš„æµ‹è¯•å‡½æ•°
def test_enhanced_scheduling_metrics():
    """æµ‹è¯•å¢å¼ºçš„è°ƒåº¦æŒ‡æ ‡ç³»ç»Ÿ"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•EnhancedSchedulingMetrics...")

    try:
        # åˆ›å»ºå¢å¼ºçš„æŒ‡æ ‡ç³»ç»Ÿ
        metrics = EnhancedSchedulingMetrics(window_size=50, stability_threshold=0.1)

        # æµ‹è¯•1: åŸºæœ¬æŒ‡æ ‡æ›´æ–°
        print("\nğŸ“ æµ‹è¯•1: åŸºæœ¬æŒ‡æ ‡æ›´æ–°æµ‹è¯•")
        for i in range(100):
            makespan = np.random.uniform(1.0, 3.0) + np.sin(i/10) * 0.5  # æ¨¡æ‹Ÿæ”¶æ•›
            reward = np.random.uniform(200, 500) + i * 2  # æ¨¡æ‹Ÿæ”¹è¿›
            load_balance = np.random.uniform(0.7, 1.0)
            energy = np.random.uniform(100, 300)
            throughput = np.random.uniform(5, 15)
            success_rate = np.random.uniform(0.8, 1.0)

            metrics.update_metrics(makespan, load_balance, energy, throughput, reward, success_rate, i)

        print(f"âœ… å·²æ›´æ–°100ä¸ªepisodeçš„æŒ‡æ ‡")

        # æµ‹è¯•2: ç»¼åˆæŒ‡æ ‡è·å–
        print("\nğŸ“ æµ‹è¯•2: ç»¼åˆæŒ‡æ ‡è·å–æµ‹è¯•")
        comprehensive = metrics.get_comprehensive_metrics()
        print(f"âœ… å¹³å‡Makespan: {comprehensive['performance']['avg_makespan']:.3f}")
        print(f"âœ… ç¨³å®šæ€§è¯„åˆ†: {comprehensive['stability']['makespan_stability']:.3f}")
        print(f"âœ… æ”¶æ•›çŠ¶æ€: {comprehensive['convergence']['converged']}")

        # æµ‹è¯•3: åŸºçº¿æ¯”è¾ƒ
        print("\nğŸ“ æµ‹è¯•3: åŸºçº¿æ¯”è¾ƒæµ‹è¯•")
        for _ in range(20):
            metrics.add_baseline_comparison('HEFT', np.random.uniform(2.5, 4.0))
            metrics.add_baseline_comparison('FCFS', np.random.uniform(3.0, 5.0))
            metrics.add_baseline_comparison('Random', np.random.uniform(4.0, 6.0))

        print("âœ… åŸºçº¿æ¯”è¾ƒæ•°æ®å·²æ·»åŠ ")

        # æµ‹è¯•4: å¯è§†åŒ–
        print("\nğŸ“ æµ‹è¯•4: å¯è§†åŒ–æµ‹è¯•")
        metrics.create_comprehensive_visualization("results/test_enhanced_metrics.png", show_plot=False)
        print("âœ… ç»¼åˆå¯è§†åŒ–å·²ç”Ÿæˆ")

        # æµ‹è¯•5: æ•°æ®å¯¼å‡º
        print("\nğŸ“ æµ‹è¯•5: æ•°æ®å¯¼å‡ºæµ‹è¯•")
        metrics.export_metrics_to_csv("results/test_metrics.csv")
        metrics.export_summary_to_json("results/test_summary.json")
        print("âœ… æ•°æ®å¯¼å‡ºå®Œæˆ")

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼EnhancedSchedulingMetricså·¥ä½œæ­£å¸¸")
        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_enhanced_scheduling_metrics()
    if success:
        print("\nâœ… Enhanced Scheduling Metrics ready for production!")
        print("ğŸ¯ ä¸»è¦å¢å¼º:")
        print("  - ç¨³å®šæ€§æŒ‡æ ‡ç›‘æ§: å®æ—¶è·Ÿè¸ªè®­ç»ƒç¨³å®šæ€§")
        print("  - æ”¶æ•›æ€§åˆ†æ: æ™ºèƒ½æ”¶æ•›æ£€æµ‹å’Œåˆ†æ")
        print("  - å¼‚å¸¸æ£€æµ‹: è‡ªåŠ¨è¯†åˆ«æ€§èƒ½å¼‚å¸¸")
        print("  - ç»¼åˆå¯è§†åŒ–: å¤šç»´åº¦æ€§èƒ½åˆ†æå›¾è¡¨")
        print("  - åŸºçº¿æ¯”è¾ƒ: ä¸ä¼ ç»Ÿç®—æ³•çš„æ€§èƒ½å¯¹æ¯”")
        print("  - æ•°æ®å¯¼å‡º: å®Œæ•´çš„æŒ‡æ ‡æ•°æ®å¯¼å‡ºåŠŸèƒ½")
    else:
        print("\nâŒ Enhanced Scheduling Metrics need debugging!")