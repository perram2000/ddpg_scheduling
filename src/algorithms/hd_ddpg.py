
"""
Hierarchical Deep Deterministic Policy Gradient (HD-DDPG)
åˆ†å±‚æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¸»å®ç°
- ç»Ÿä¸€ç¯å¢ƒæ”¯æŒ
- stateçŠ¶æ€è·å–
- è°ƒåº¦é€»è¾‘
- å¥–åŠ±å‡½æ•°
- ä¸åŸºçº¿ç®—æ³•çš„å…¬å¹³æ¯”è¾ƒ
"""

import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Optional, Any
import time
from collections import deque

from .meta_controller import StabilizedMetaController
from .sub_controllers import EnhancedSubControllerManager
from ..utils.replay_buffer import OptimizedHierarchicalReplayBuffer


class OptimizedConvergenceMonitor:
    """ä¼˜åŒ–çš„æ”¶æ•›ç›‘æ§å™¨ - æ›´æ•æ„Ÿçš„æ—©åœæœºåˆ¶"""

    def __init__(self, patience=80, min_delta=0.01):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = float('inf')
        self.counter = 0
        self.early_stop = False
        self.improvement_history = []
        self.consecutive_improvements = 0

    def __call__(self, current_score):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        improvement = self.best_score - current_score

        if improvement > self.min_delta:
            self.best_score = current_score
            self.counter = 0
            self.consecutive_improvements += 1
            self.improvement_history.append(improvement)

            # å¿«é€Ÿæ”¶æ•›æ£€æµ‹ï¼ˆæ”¾å®½ï¼Œé¿å…è¯¯æŠ¥ï¼‰ï¼ˆMODï¼‰
            if self.consecutive_improvements >= 8:
                recent = self.improvement_history[-8:]
                recent_avg = np.mean(recent) if recent else 0.0
                if recent_avg < self.min_delta * 0.6:
                    # ä»…æ ‡è®°ä¸ºå¯æ—©åœï¼Œä¸ç«‹å³å¼ºåˆ¶
                    self.early_stop = False
        else:
            self.counter += 1
            self.consecutive_improvements = 0

        if self.counter >= self.patience:
            self.early_stop = True

        return self.early_stop

    def get_status(self):
        """è·å–æ”¶æ•›çŠ¶æ€"""
        return {
            'early_stop': self.early_stop,
            'best_score': self.best_score,
            'patience_counter': self.counter,
            'consecutive_improvements': self.consecutive_improvements,
            'improvement_trend': np.mean(self.improvement_history[-10:]) if len(self.improvement_history) >= 10 else 0
        }


class SimplifiedLearningRateScheduler:
    """ç®€åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, initial_lr=0.0005, decay_factor=0.9, decay_frequency=80, min_lr=0.0001):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.decay_factor = decay_factor
        self.decay_frequency = decay_frequency
        self.min_lr = min_lr
        self.last_decay_episode = 0
        self.performance_history = []
        self.total_updates = 0  # ä»…ä¿ç•™è¿™ä¸€è¡Œ

    def update(self, episode, performance_metric):
        """æ›´æ¸©å’Œçš„è¡°å‡ï¼šè¶‹åŠ¿ç»å¯¹å€¼ < 0.05 æ‰è§¦å‘ï¼Œä¸”è®°å½• total_updates"""
        self.performance_history.append(performance_metric)

        if episode - self.last_decay_episode >= self.decay_frequency:
            if len(self.performance_history) >= 20:
                recent = self.performance_history[-20:]
                trend = np.polyfit(range(20), recent, 1)[0]
                if abs(trend) < 0.05:
                    new_lr = max(self.current_lr * self.decay_factor, self.min_lr)
                    if new_lr < self.current_lr - 1e-12:
                        self.current_lr = new_lr
                        self.last_decay_episode = episode
                        self.total_updates += 1
                        print(f"INFO: Episode {episode}: LR decayed to {self.current_lr:.6f}")
        return self.current_lr


class StreamlinedMetrics:
    """æµçº¿å‹æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨"""

    def __init__(self, smoothing_window=10):
        self.smoothing_window = smoothing_window
        self.metrics_history = {
            'makespan': deque(maxlen=500),
            'load_balance': deque(maxlen=500),
            'energy': deque(maxlen=500),
            'throughput': deque(maxlen=500),
            'reward': deque(maxlen=500)
        }
        self.smoothed_metrics = {
            'makespan': deque(maxlen=smoothing_window),
            'load_balance': deque(maxlen=smoothing_window),
            'reward': deque(maxlen=smoothing_window)
        }

    def update_metrics(self, makespan, load_balance, energy, throughput, reward):
        """ç®€åŒ–çš„æŒ‡æ ‡æ›´æ–°"""
        try:
            # ğŸ¯ ä¸¥æ ¼çš„makespanå¼‚å¸¸å€¼å¤„ç†
            if makespan != float('inf') and not np.isnan(makespan):
                if len(self.metrics_history['makespan']) >= 5:
                    recent_makespans = list(self.metrics_history['makespan'])[-5:]
                    mean_recent = np.mean(recent_makespans)
                    std_recent = np.std(recent_makespans)

                    if std_recent > 0 and abs(makespan - mean_recent) > 3 * std_recent:
                        makespan = mean_recent + np.sign(makespan - mean_recent) * 2 * std_recent

            # æ›´æ–°å†å²è®°å½•
            self.metrics_history['makespan'].append(makespan)
            self.metrics_history['load_balance'].append(load_balance)
            self.metrics_history['energy'].append(energy)
            self.metrics_history['throughput'].append(throughput)
            self.metrics_history['reward'].append(reward)

            # å•å±‚å¹³æ»‘æ›´æ–°
            self.smoothed_metrics['makespan'].append(makespan)
            self.smoothed_metrics['load_balance'].append(load_balance)
            self.smoothed_metrics['reward'].append(reward)

        except Exception as e:
            print(f"WARNING: Metrics update error: {e}")

    def get_smoothed_makespan(self):
        """è·å–å¹³æ»‘makespan"""
        if self.smoothed_metrics['makespan']:
            return np.mean(list(self.smoothed_metrics['makespan']))
        return float('inf')

    def get_makespan_stability(self):
        """è®¡ç®—makespanç¨³å®šæ€§"""
        try:
            if len(self.smoothed_metrics['makespan']) >= 5:
                recent_makespans = list(self.smoothed_metrics['makespan'])
                variance = np.var(recent_makespans)
                stability = 1.0 / (1.0 + variance)
                return max(0.1, min(1.0, stability))
            return 1.0
        except Exception:
            return 0.5

    def get_trend_direction(self):
        """è·å–æ€§èƒ½è¶‹åŠ¿æ–¹å‘"""
        try:
            if len(self.metrics_history['makespan']) >= 10:
                recent_makespans = list(self.metrics_history['makespan'])[-10:]
                trend = np.polyfit(range(len(recent_makespans)), recent_makespans, 1)[0]

                if trend < -0.1:
                    return 'improving'
                elif trend > 0.1:
                    return 'degrading'
                else:
                    return 'stable'
            return 'insufficient_data'
        except Exception:
            return 'unknown'


class HDDDPG:
    """
    HD-DDPGä¸»ç®—æ³•ç±» - ç»Ÿä¸€ç¯å¢ƒç‰ˆæœ¬
    ğŸ¯ æ–°å¢åŠŸèƒ½ï¼šæ”¯æŒç»Ÿä¸€è¯„ä¼°ç¯å¢ƒçš„è°ƒåº¦æ–¹æ³•
    """

    def __init__(self, config: Dict = None):
        # ğŸš€ é»˜è®¤é…ç½® - å°†è¢«å¤–éƒ¨configè¦†ç›–
        self.config = {
            # æ ¸å¿ƒå‚æ•°
            'meta_state_dim': 15,
            'meta_action_dim': 3,
            'gamma': 0.95,
            'batch_size': 32,
            'memory_capacity': 10000,
            'meta_lr': 0.0005,
            'sub_lr': 0.0004,
            'tau': 0.008,
            'update_frequency': 1,
            'save_frequency': 25,

            # æ¢ç´¢å‚æ•°
            'gradient_clip_norm': 0.5,
            'exploration_noise': 0.12,
            'noise_decay': 0.995,
            'min_noise': 0.02,
            'reward_scale': 1.0,
            'verbose': False,

            # ğŸ¯ ä¿®å¤ï¼šå¥–åŠ±æƒé‡é…ç½®
            'action_smoothing': True,
            'action_smoothing_alpha': 0.15,
            'makespan_weight': 0.90,  # ä¸»å¯¼æƒé‡ä½†ä¸è¿‡åº¦
            'stability_weight': 0.05,
            'quality_weight': 0.05,
            'target_update_frequency': 2,

            # ğŸ¯ ä¿®å¤ï¼šmakespanç›®æ ‡é…ç½®
            'makespan_target': 8.0,  # é»˜è®¤å€¼ï¼Œå°†è¢«å¤–éƒ¨è¦†ç›–
            'makespan_improvement_bonus': 6.0,

            # å…¶ä»–å‚æ•°
            'enable_per': True,
            'quality_threshold': 0.08,
            'td_error_clipping': True,
            'td_error_max': 1.0,
            'adaptive_learning': True,
            'learning_rate_decay': True,
            'lr_decay_factor': 0.995,
            'lr_decay_frequency': 150,

            'makespan_smoothing': True,
            'makespan_smoothing_window': 5,
            'multi_level_smoothing': False,
            'rebound_detection': False,

            'convergence_patience': 60,
            'convergence_threshold': 0.05,
            'early_stopping': True,
            'plateau_detection': False,

            'track_losses': True,
            'loss_logging_frequency': 5,
            'save_loss_plots': True,
            'force_internal_reward_scaling': True,
        }

        # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿å¤–éƒ¨é…ç½®æ­£ç¡®è¦†ç›–é»˜è®¤é…ç½®
        if config:
            print("INFO: Applying external configuration...")
            for key, value in config.items():
                if key in self.config:
                    old_value = self.config[key]
                    self.config[key] = value
                    if key in ['makespan_target', 'makespan_weight', 'batch_size', 'meta_lr']:
                        print(f"INFO: {key}: {old_value} -> {value}")
                else:
                    self.config[key] = value
                # MOD: å›ºå®šé€šä¿¡æƒé‡é»˜è®¤å€¼ï¼ˆå¯è¢«å¤–éƒ¨è¦†ç›–ï¼‰
                if 'comm_weight' not in self.config or self.config['comm_weight'] is None:
                    self.config['comm_weight'] = 0.40
                # å¯é€‰å¼ºåˆ¶ç­–ç•¥ï¼šä¼˜å…ˆå†…éƒ¨æ¨èçš„å¥–åŠ±å°ºåº¦
                if self.config.get('force_internal_reward_scaling', True):
                    self.config['reward_scale'] = 1.0
                    self.config['makespan_improvement_bonus'] = 6.0
                    print("INFO: Forced internal reward scaling: reward_scale=1.0, makespan_improvement_bonus=6.0")

        print("INFO: Initializing HD-DDPG algorithm (Complete Fix Version)")
        print(f"INFO: ğŸ¯ Final makespan target: {self.config['makespan_target']}")
        print(f"INFO: ğŸ¯ Final makespan weight: {self.config['makespan_weight']}")
        print(f"INFO: ğŸ¯ Final batch size: {self.config['batch_size']}")
        print(f"INFO: ğŸ¯ Final learning rate: {self.config['meta_lr']}")
        print(f"INFO: ğŸ”§ Communication weight (comm_weight): {self.config['comm_weight']}")
        print("INFO: HD-DDPG complete fix version initialization completed")

        # åˆå§‹åŒ–ç»„ä»¶
        self.convergence_monitor = OptimizedConvergenceMonitor(
            patience=self.config.get('convergence_patience', 60),
            min_delta=self.config.get('convergence_threshold', 0.05)
        )

        self.lr_scheduler = SimplifiedLearningRateScheduler(
            initial_lr=self.config.get('meta_lr', 0.0005),
            decay_factor=self.config.get('lr_decay_factor', 0.995),
            decay_frequency=self.config.get('lr_decay_frequency', 150),
            min_lr=self.config.get('min_lr', 0.0001)
        )

        # çŠ¶æ€å˜é‡
        self.is_converged = False
        self.best_performance = float('inf')
        self.makespan_history = deque(maxlen=100)

        # åŠ¨ä½œå¹³æ»‘åŒ–ç¼“å†²åŒº
        self.previous_actions = {
            'meta': None,
            'FPGA': None,
            'FOG_GPU': None,
            'CLOUD': None
        }

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        try:
            self.meta_controller = StabilizedMetaController(
                state_dim=self.config['meta_state_dim'],
                action_dim=self.config['meta_action_dim'],
                learning_rate=self.config['meta_lr']
            )
            print("INFO: Meta controller initialized successfully")

            self.sub_controller_manager = EnhancedSubControllerManager(
                sub_learning_rate=self.config['sub_lr']
            )
            print("INFO: Sub controller manager initialized successfully")

            self.replay_buffer = OptimizedHierarchicalReplayBuffer(
                capacity=self.config['memory_capacity'],
                enable_per=self.config['enable_per'],
                quality_threshold=self.config['quality_threshold'],
                balance_sampling=True,
                priority_alpha=0.6
            )
            print("INFO: Replay buffer initialized successfully")

            self.metrics = StreamlinedMetrics(
                smoothing_window=self.config['makespan_smoothing_window']
            )
            print("INFO: Streamlined metrics calculator initialized successfully")

        except Exception as e:
            print(f"ERROR: HD-DDPG initialization failed: {e}")
            raise

        # è®­ç»ƒçŠ¶æ€
        self.episode = 0
        self.total_steps = 0
        self.update_counter = 0

        # è®­ç»ƒå†å²
        self.training_history = {
            'episode_rewards': [],
            'makespans': [],
            'load_balances': [],
            'makespan_stability': [],
        }

        # å¥–åŠ±ç»Ÿè®¡
        self.reward_stats = {
            'recent_rewards': deque(maxlen=50),
            'makespan_history': deque(maxlen=30),
        }

        print("INFO: HD-DDPG complete fix version initialization completed")

    # ===== ğŸ†• æ–°å¢ï¼šç»Ÿä¸€ç¯å¢ƒè°ƒåº¦æ–¹æ³• =====
    def schedule_workflow_unified(self, workflow: List, environment) -> Dict[str, Any]:
        """ç»Ÿä¸€ç¯å¢ƒçš„å·¥ä½œæµè°ƒåº¦æ–¹æ³•"""
        environment.reset()

        task_assignments = []
        total_energy = 0.0

        for task in workflow:
            try:
                # è·å–ç³»ç»ŸçŠ¶æ€
                system_state = environment.get_system_state()

                # ä½¿ç”¨å…ƒæ§åˆ¶å™¨é€‰æ‹©é›†ç¾¤
                meta_action = self.meta_controller.get_action(system_state, training=False)
                cluster_choice = np.argmax(meta_action)
                cluster_names = ['FPGA', 'FOG_GPU', 'CLOUD']
                selected_cluster = cluster_names[cluster_choice]

                # åœ¨é€‰å®šé›†ç¾¤ä¸­é€‰æ‹©æœ€ä½³èŠ‚ç‚¹
                cluster_nodes = environment.get_cluster_nodes(selected_cluster)

                if cluster_nodes:
                    # æ‰¾åˆ°å¯ç”¨ä¸”æœ€ä¼˜çš„èŠ‚ç‚¹
                    available_nodes = [n for n in cluster_nodes if n.can_accommodate(task.memory_requirement)]

                    if available_nodes:
                        best_node = min(available_nodes, key=lambda n: n.completion_time)
                        assignment_result = best_node.add_task(task)

                        task_assignments.append({
                            'task_id': task.task_id,
                            'node_id': best_node.node_id,
                            'cluster_type': best_node.cluster_type,
                            'completion_time': assignment_result['completion_time'],
                            'energy_consumption': assignment_result['energy_consumption']
                        })

                        total_energy += assignment_result['energy_consumption']
                    else:
                        # å¦‚æœé€‰å®šé›†ç¾¤æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œå°è¯•å…¶ä»–é›†ç¾¤
                        all_nodes = environment.get_all_nodes()
                        available_nodes = [n for n in all_nodes if n.can_accommodate(task.memory_requirement)]

                        if available_nodes:
                            best_node = min(available_nodes, key=lambda n: n.completion_time)
                            assignment_result = best_node.add_task(task)

                            task_assignments.append({
                                'task_id': task.task_id,
                                'node_id': best_node.node_id,
                                'cluster_type': best_node.cluster_type,
                                'completion_time': assignment_result['completion_time'],
                                'energy_consumption': assignment_result['energy_consumption']
                            })

                            total_energy += assignment_result['energy_consumption']

            except Exception as e:
                print(f"âš ï¸ Task {task.task_id} scheduling failed: {e}")
                continue

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        if task_assignments:
            makespan = max(task['completion_time'] for task in task_assignments)
            success_rate = len(task_assignments) / len(workflow)

            # è®¡ç®—è´Ÿè½½å‡è¡¡
            cluster_loads = {}
            for task in task_assignments:
                cluster = task['cluster_type']
                cluster_loads.setdefault(cluster, []).append(task['completion_time'])

            if cluster_loads:
                cluster_max_times = [max(times) for times in cluster_loads.values()]
                avg_time = sum(cluster_max_times) / len(cluster_max_times)
                load_variance = sum((t - avg_time) ** 2 for t in cluster_max_times) / len(cluster_max_times)
                load_balance = 1.0 / (1.0 + load_variance)
            else:
                load_balance = 0.0
        else:
            makespan = float('inf')
            success_rate = 0.0
            load_balance = 0.0

        return {
            'makespan': makespan,
            'load_balance': load_balance,
            'total_energy': total_energy,
            'completed_tasks': len(task_assignments),
            'success_rate': success_rate,
            'task_assignments': task_assignments
        }

    def _get_meta_action(self, system_state: np.ndarray) -> np.ndarray:
        """è·å–å…ƒæ§åˆ¶å™¨åŠ¨ä½œ"""
        try:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„å…ƒæ§åˆ¶å™¨
            if hasattr(self.meta_controller, 'actor'):
                state_tensor = tf.expand_dims(system_state, 0)
                action = self.meta_controller.actor(state_tensor, training=False)
                action_np = action.numpy()[0]

                # åŠ¨ä½œå¹³æ»‘
                if self.config.get('action_smoothing', True):
                    action_np = self._simple_smooth_action(action_np, 'meta')

                return action_np
        except Exception as e:
            print(f"WARNING: Meta controller action failed: {e}")

        # å¯å‘å¼å¤‡é€‰æ–¹æ¡ˆï¼šåŸºäºç³»ç»ŸçŠ¶æ€çš„æ™ºèƒ½é€‰æ‹©
        return self._heuristic_cluster_selection(system_state)

    def _heuristic_cluster_selection(self, system_state: np.ndarray) -> np.ndarray:
        """å¯å‘å¼é›†ç¾¤é€‰æ‹©"""
        try:
            # system_state[1:4] æ˜¯å„é›†ç¾¤åˆ©ç”¨ç‡
            if len(system_state) >= 4:
                cluster_utilizations = system_state[1:4]

                # é€‰æ‹©åˆ©ç”¨ç‡æœ€ä½çš„é›†ç¾¤
                best_cluster_idx = np.argmin(cluster_utilizations)

                # è½¬æ¢ä¸ºåŠ¨ä½œæ ¼å¼
                action = np.zeros(3)
                action[best_cluster_idx] = 1.0

                return action
        except Exception:
            pass

        # é»˜è®¤é€‰æ‹©FOG_GPU
        action = np.array([0.2, 0.6, 0.2])
        return action

    def _action_to_cluster(self, action: np.ndarray) -> str:
        """åŠ¨ä½œè½¬æ¢ä¸ºé›†ç¾¤ç±»å‹"""
        cluster_mapping = ['FPGA', 'FOG_GPU', 'CLOUD']
        cluster_idx = np.argmax(action)
        return cluster_mapping[cluster_idx]

    def sub_controllers(self):
        """è·å–å­æ§åˆ¶å™¨çš„å±æ€§è®¿é—®å™¨"""
        if hasattr(self, 'sub_controller_manager') and self.sub_controller_manager:
            return {
                'FPGA': self.sub_controller_manager.fpga_controller,
                'FOG_GPU': self.sub_controller_manager.fog_gpu_controller,
                'CLOUD': self.sub_controller_manager.cloud_controller
            }
        return {}

    def _get_sub_action(self, cluster_type: str, cluster_state: np.ndarray) -> np.ndarray:
        """è·å–å­æ§åˆ¶å™¨åŠ¨ä½œ"""
        try:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„å­æ§åˆ¶å™¨
            if hasattr(self.sub_controller_manager, 'sub_controllers'):
                sub_controllers = self.sub_controller_manager.sub_controllers
                if cluster_type in sub_controllers:
                    sub_controller = sub_controllers[cluster_type]
                    if hasattr(sub_controller, 'actor'):
                        state_tensor = tf.expand_dims(cluster_state, 0)
                        action = sub_controller.actor(state_tensor, training=False)
                        action_np = action.numpy()[0]

                        # åŠ¨ä½œå¹³æ»‘
                        if self.config.get('action_smoothing', True):
                            action_np = self._simple_smooth_action(action_np, cluster_type)

                        return action_np
        except Exception as e:
            print(f"WARNING: Sub controller action failed ({cluster_type}): {e}")

        # å¯å‘å¼å¤‡é€‰æ–¹æ¡ˆ
        return self._heuristic_node_selection(cluster_state)

    def _heuristic_node_selection(self, cluster_state: np.ndarray) -> np.ndarray:
        """å¯å‘å¼èŠ‚ç‚¹é€‰æ‹©"""
        try:
            if len(cluster_state) >= 2:
                memory_util = cluster_state[0]
                availability = cluster_state[1]

                if availability > 0.7:  # é«˜å¯ç”¨æ€§ï¼Œé€‰æ‹©è´Ÿè½½å‡è¡¡
                    return np.array([0.5, 0.5])
                else:  # ä½å¯ç”¨æ€§ï¼Œé€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
                    return np.array([1.0, 0.0])
        except Exception:
            pass

        return np.array([0.5, 0.5])

    def _select_node_from_unified_action(self, cluster_nodes: List, action: np.ndarray, task) -> Optional:
        """ä»ç»Ÿä¸€ç¯å¢ƒçš„åŠ¨ä½œé€‰æ‹©å…·ä½“èŠ‚ç‚¹"""
        try:
            # è¿‡æ»¤å¯ç”¨èŠ‚ç‚¹
            available_nodes = [node for node in cluster_nodes
                              if node.can_accommodate(task.memory_requirement)]

            if not available_nodes:
                return None

            # è§£æåŠ¨ä½œåå¥½
            if len(action) >= 2:
                balance_preference = action[0]  # è´Ÿè½½å‡è¡¡åå¥½
                performance_preference = action[1]  # æ€§èƒ½åå¥½
            else:
                balance_preference = 0.5
                performance_preference = 0.5

            # è®¡ç®—èŠ‚ç‚¹è¯„åˆ†
            best_node = None
            best_score = -1

            for node in available_nodes:
                # è´Ÿè½½å‡è¡¡è¯„åˆ†
                load_ratio = node.current_load / node.memory_capacity
                balance_score = 1.0 - load_ratio

                # æ€§èƒ½è¯„åˆ†
                performance_score = node.processing_speed * node.energy_efficiency
                performance_score = min(performance_score, 3.0)  # å½’ä¸€åŒ–

                # ç»¼åˆè¯„åˆ†
                total_score = (balance_preference * balance_score +
                              performance_preference * performance_score)

                if total_score > best_score:
                    best_score = total_score
                    best_node = node

            return best_node

        except Exception as e:
            print(f"WARNING: Node selection error: {e}")
            # è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
            available_nodes = [node for node in cluster_nodes
                              if node.can_accommodate(task.memory_requirement)]
            return available_nodes[0] if available_nodes else None

    def _backup_assignment_unified(self, task, environment) -> Optional[Dict]:
        """ç»Ÿä¸€ç¯å¢ƒçš„å¤‡é€‰ä»»åŠ¡åˆ†é…æ–¹æ¡ˆ"""
        try:
            # å°è¯•æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
            all_nodes = environment.get_all_nodes()

            best_node = None
            best_completion_time = float('inf')

            for node in all_nodes:
                if node.can_accommodate(task.memory_requirement):
                    exec_time = node.get_execution_time(task.computation_requirement)
                    completion_time = node.completion_time + exec_time

                    if completion_time < best_completion_time:
                        best_completion_time = completion_time
                        best_node = node

            if best_node:
                assignment_result = best_node.add_task(task)
                return {
                    'task_id': task.task_id,
                    'node_id': best_node.node_id,
                    'cluster_type': best_node.cluster_type,
                    'start_time': assignment_result['start_time'],
                    'completion_time': assignment_result['completion_time'],
                    'execution_time': assignment_result['execution_time'],
                    'energy_consumption': assignment_result['energy_consumption']
                }

            return None

        except Exception as e:
            print(f"WARNING: Backup assignment error: {e}")
            return None

    def _calculate_unified_task_reward(self, assignment_result: Dict, task) -> float:
        """è®¡ç®—ç»Ÿä¸€ç¯å¢ƒä¸‹çš„ä»»åŠ¡å¥–åŠ±"""
        try:
            execution_time = assignment_result.get('execution_time', 1.0)
            energy_consumption = assignment_result.get('energy_consumption', 10.0)

            # æ‰§è¡Œæ—¶é—´å¥–åŠ±ï¼ˆä¸»è¦å› ç´ ï¼‰
            if execution_time <= 0.5:
                time_reward = 10.0
            elif execution_time <= 1.0:
                time_reward = 10.0 - 8.0 * (execution_time - 0.5) / 0.5
            elif execution_time <= 2.0:
                time_reward = 2.0 - 2.0 * (execution_time - 1.0) / 1.0
            else:
                time_reward = max(-3.0, -execution_time * 0.5)

            # èƒ½æ•ˆå¥–åŠ±ï¼ˆæ¬¡è¦å› ç´ ï¼‰
            energy_reward = max(0.0, 3.0 - energy_consumption * 0.05)

            total_reward = time_reward + energy_reward * 0.3
            return np.clip(total_reward, -5.0, 15.0)

        except Exception:
            return 0.0

    def _calculate_unified_final_metrics(self, task_assignments: List[Dict],
                                       total_energy: float, total_reward: float,
                                       total_tasks: int, failed_tasks: int) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿä¸€ç¯å¢ƒçš„æœ€ç»ˆæŒ‡æ ‡"""
        try:
            if not task_assignments:
                return self._create_unified_failed_result(total_tasks)

            # è®¡ç®—makespan
            completion_times = [assignment['completion_time'] for assignment in task_assignments]
            makespan = max(completion_times)

            # è®¡ç®—è´Ÿè½½å‡è¡¡ï¼ˆåŸºäºä»»åŠ¡åˆ†å¸ƒï¼‰
            cluster_counts = {}
            for assignment in task_assignments:
                cluster = assignment['cluster_type']
                cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1

            if len(cluster_counts) > 1:
                task_counts = list(cluster_counts.values())
                load_balance = 1.0 - (np.std(task_counts) / np.mean(task_counts))
                load_balance = max(0, min(1, load_balance))
            else:
                load_balance = 0.8  # å•é›†ç¾¤ä½¿ç”¨çš„åˆç†è¯„åˆ†

            # æˆåŠŸç‡
            success_rate = len(task_assignments) / total_tasks

            # å·¥ä½œæµçº§åˆ«çš„makespanå¥–åŠ±
            makespan_target = self.config.get('makespan_target', 8.0)
            if makespan <= makespan_target:
                makespan_bonus = 14.0
            elif makespan <= makespan_target * 1.2:
                gap_ratio = (makespan - makespan_target) / (makespan_target * 0.2)
                makespan_bonus = 14.0 * (1.0 - gap_ratio)
            else:
                makespan_bonus = 0.0

            total_reward += makespan_bonus

            return {
                'makespan': makespan,
                'load_balance': load_balance,
                'total_energy': total_energy,
                'completed_tasks': len(task_assignments),
                'success_rate': success_rate,
                'total_reward': total_reward
            }

        except Exception as e:
            print(f"WARNING: Unified final metrics calculation error: {e}")
            return self._create_unified_failed_result(total_tasks)

    def _create_unified_failed_result(self, total_tasks: int) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿä¸€ç¯å¢ƒçš„å¤±è´¥ç»“æœ"""
        return {
            'makespan': float('inf'),
            'load_balance': 0.0,
            'total_energy': 0.0,
            'completed_tasks': 0,
            'success_rate': 0.0,
            'total_reward': -50.0
        }

    # ===== ğŸ”„ ä¿æŒåŸæœ‰çš„è®­ç»ƒæ–¹æ³• =====
    def _update_learning_rates(self, new_lr):
        """åŒæ­¥åˆ°ä¼˜åŒ–å™¨ + è®°å½•å½“å‰LRåˆ°è°ƒåº¦å™¨ä¸é…ç½®"""
        try:
            # è®°å½•æ—§å€¼ï¼Œä¾¿äºåˆ¤æ–­æ˜¯å¦è¡°å‡
            old_lr = float(self.lr_scheduler.current_lr)

            # å…ƒæ§åˆ¶å™¨
            if hasattr(self.meta_controller, 'actor_optimizer'):
                try:
                    self.meta_controller.actor_optimizer.learning_rate.assign(new_lr)
                except AttributeError:
                    self.meta_controller.actor_optimizer.lr = new_lr
            if hasattr(self.meta_controller, 'critic_optimizer'):
                try:
                    self.meta_controller.critic_optimizer.learning_rate.assign(new_lr * 1.2)
                except AttributeError:
                    self.meta_controller.critic_optimizer.lr = new_lr * 1.2

            # å­æ§åˆ¶å™¨
            if hasattr(self.sub_controller_manager, 'update_learning_rates'):
                self.sub_controller_manager.update_learning_rates(new_lr * 0.8)

            # è®°è´¦åŒæ­¥
            self.lr_scheduler.current_lr = float(new_lr)
            self.config['meta_lr'] = float(new_lr)
            self.lr_scheduler.last_decay_episode = self.episode

            # è‹¥ç¡®å®é™ä½ï¼Œåˆ™è®¡æ•°+1ï¼ˆå¤–éƒ¨/å†…éƒ¨å‡ç»Ÿè®¡ï¼‰
            if new_lr < old_lr - 1e-12:
                self.lr_scheduler.total_updates += 1

            print(f"INFO: Learning rate updated to: {new_lr:.6f}")
        except Exception as e:
            print(f"WARNING: Learning rate update failed: {e}")

    def update_learning_rate(self, new_lr: float):
        """å¯¹å¤–ç»Ÿä¸€æ¥å£ï¼Œä¾›è®­ç»ƒè„šæœ¬/è°ƒåº¦å™¨è°ƒç”¨ï¼ˆMODï¼‰"""
        self._update_learning_rates(new_lr)

    def _check_convergence_status(self, episode_makespan):
        """æ£€æŸ¥æ”¶æ•›çŠ¶æ€"""
        try:
            should_stop = self.convergence_monitor(episode_makespan)

            if episode_makespan < self.best_performance:
                self.best_performance = episode_makespan

            makespan_target = self.config.get('makespan_target', 8.0)
            if episode_makespan <= makespan_target:
                print(f"INFO: ğŸ¯ Makespan target achieved! Current: {episode_makespan:.3f}, Target: {makespan_target}")

            if should_stop and self.config.get('early_stopping', True):
                self.is_converged = True
                print(f"INFO: Convergence detected, best makespan: {self.best_performance:.4f}")

            return should_stop
        except Exception as e:
            print(f"WARNING: Convergence status check failed: {e}")
            return False

    def _calculate_progressive_reward(self, task, execution_result, environment, completion_times) -> float:
        """
        ğŸ¯ æ¸è¿›å¼å¥–åŠ±å‡½æ•° - ä¿®å¤äºŒå…ƒåŒ–å¥–åŠ±é—®é¢˜
        æä¾›å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œé¿å…ç¨€ç–å¥–åŠ±
        """
        try:
            if not execution_result.get('success', True):
                return -5.0  # å¤±è´¥æƒ©ç½š

            execution_time = execution_result.get('execution_time', 0)
            energy_consumption = execution_result.get('energy_consumption', 0)
            quality_score = execution_result.get('quality_score', 1.0)

            # ğŸ¯ è·å–makespanç›®æ ‡
            makespan_target = self.config.get('makespan_target', 8.0)

            # ğŸ¯ åŸºç¡€æ‰§è¡Œæ—¶é—´å¥–åŠ±ï¼ˆ0-15åˆ†ï¼‰- ä¸»è¦ä¿¡å·
            if execution_time <= 0.3:
                time_reward = 15.0  # æå¿«æ‰§è¡Œ
            elif execution_time <= 0.8:
                time_reward = 15.0 - 10.0 * (execution_time - 0.3) / 0.5  # çº¿æ€§é€’å‡
            elif execution_time <= 1.5:
                time_reward = 5.0 - 3.0 * (execution_time - 0.8) / 0.7  # ç»§ç»­é€’å‡
            elif execution_time <= 3.0:
                time_reward = 2.0 - 2.0 * (execution_time - 1.5) / 1.5  # æ¥è¿‘0
            else:
                time_reward = max(-2.0, -execution_time * 0.5)  # è´Ÿå¥–åŠ±

            # ğŸ¯ æ¸è¿›å¼makespanå¥–åŠ±ï¼ˆ-5åˆ°+25åˆ†ï¼‰- å…³é”®æ”¹è¿›
            makespan_reward = 0.0
            if completion_times:
                current_makespan = max(completion_times)

                if current_makespan <= makespan_target:
                    # ğŸ‰ è¾¾åˆ°ç›®æ ‡ï¼šå¤§å¥–åŠ±
                    exceed_ratio = (makespan_target - current_makespan) / makespan_target
                    makespan_reward = 25.0 + 10.0 * exceed_ratio  # 25-35åˆ†

                elif current_makespan <= makespan_target * 1.2:
                    # ğŸ¯ æ¥è¿‘ç›®æ ‡ï¼šæ¸è¿›å¥–åŠ±ï¼ˆ120%ä»¥å†…ï¼‰
                    gap_ratio = (current_makespan - makespan_target) / (makespan_target * 0.2)
                    makespan_reward = 15.0 * (1.0 - gap_ratio)  # 0-15åˆ†

                elif current_makespan <= makespan_target * 1.5:
                    # âš ï¸ ç¨è¿œç›®æ ‡ï¼šå°å¥–åŠ±ï¼ˆ150%ä»¥å†…ï¼‰
                    gap_ratio = (current_makespan - makespan_target * 1.2) / (makespan_target * 0.3)
                    makespan_reward = 8.0 * (1.0 - gap_ratio)  # 0-8åˆ†

                elif current_makespan <= makespan_target * 2.0:
                    # ğŸ˜ è¿œç¦»ç›®æ ‡ï¼šè½»å¾®æƒ©ç½šï¼ˆ200%ä»¥å†…ï¼‰
                    gap_ratio = (current_makespan - makespan_target * 1.5) / (makespan_target * 0.5)
                    makespan_reward = -3.0 * gap_ratio  # 0åˆ°-3åˆ†

                else:
                    # ğŸ˜° ä¸¥é‡è¶…æ ‡ï¼šè¾ƒå¤§æƒ©ç½š
                    makespan_reward = -5.0

            # ğŸ¯ Makespanæ”¹è¿›å¥–åŠ±ï¼ˆåŠ¨æ€å¥–åŠ±ï¼‰
            improvement_bonus = 0.0
            if len(self.makespan_history) >= 3:
                recent_makespans = list(self.makespan_history)[-3:]
                if completion_times:
                    current_makespan = max(completion_times)
                    best_recent = min(recent_makespans)
                    if current_makespan < best_recent and best_recent > 1e-8:
                        improvement_ratio = (best_recent - current_makespan) / best_recent
                        improvement_bonus = improvement_ratio * self.config.get('makespan_improvement_bonus', 6.0)

            # ğŸ¯ ç®€åŒ–çš„å…¶ä»–å¥–åŠ±
            energy_reward = max(0.0, 2.0 - energy_consumption * 0.02)  # 0-2åˆ†
            quality_reward = quality_score * 1.0  # 0-1åˆ†

            # ğŸ¯ æƒé‡ç»„åˆ
            makespan_weight = self.config.get('makespan_weight', 0.90)

            total_reward = (
                    time_reward * 0.3 +
                    makespan_reward * self.config.get('makespan_weight', 0.90) +
                    improvement_bonus +
                    energy_reward * 0.05 +
                    quality_reward * 0.05
            )
            total_reward = np.clip(total_reward, -8.0, 50.0)
            return float(total_reward)

        except Exception as e:
            print(f"WARNING: Progressive reward calculation error: {e}")
            return -3.0

    def train_episode(self, workflows: List, environment) -> Dict:
        """
        è®­ç»ƒä¸€ä¸ªepisode - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
        """
        episode_start_time = time.time()
        total_episode_reward = 0
        episode_makespans = []
        episode_load_balances = []
        all_individual_rewards = []

        # ç»éªŒå­˜å‚¨
        meta_experiences = []
        sub_experiences = {'FPGA': [], 'FOG_GPU': [], 'CLOUD': []}

        for workflow_idx, workflow in enumerate(workflows):
            try:
                result = self.schedule_workflow(workflow, environment)

                workflow_reward = result.get('total_reward', 0)
                individual_rewards = result.get('individual_rewards', [])

                all_individual_rewards.extend(individual_rewards)
                total_episode_reward += workflow_reward
                episode_makespans.append(result['makespan'])
                episode_load_balances.append(result['load_balance'])

                # ç»éªŒæ”¶é›†
                self._collect_experiences(result, meta_experiences, sub_experiences)

            except Exception as e:
                print(f"WARNING: Workflow {workflow_idx} processing error: {e}")

        # æ›´æ–°å¥–åŠ±ç»Ÿè®¡
        self.reward_stats['recent_rewards'].extend(all_individual_rewards[-20:])

        # å­˜å‚¨ç»éªŒ
        stored_count = self._store_experiences_to_buffer(meta_experiences, sub_experiences)

        # ç½‘ç»œæ›´æ–°
        training_losses = self._update_networks()

        # æ›´æ–°çŠ¶æ€
        self.episode += 1
        episode_duration = time.time() - episode_start_time

        # è®¡ç®—å¹³å‡æ€§èƒ½
        valid_makespans = [m for m in episode_makespans if m != float('inf') and not np.isnan(m)]
        avg_makespan = np.mean(valid_makespans) if valid_makespans else float('inf')

        # ğŸ¯ æ›´æ–°makespanå†å²
        if avg_makespan != float('inf'):
            self.makespan_history.append(avg_makespan)

        valid_load_balances = [lb for lb in episode_load_balances if not np.isnan(lb)]
        avg_load_balance = np.mean(valid_load_balances) if valid_load_balances else 0

        # æ”¶æ•›æ£€æµ‹
        should_stop = self._check_convergence_status(avg_makespan)
        valid_energy = [r.get('total_energy', 0.0) for r in self.simulation_results[-1:]] if hasattr(self,
                                                                                                     'simulation_results') else []
        avg_energy = np.mean(valid_energy) if valid_energy else 0.0

        # å­¦ä¹ ç‡è°ƒåº¦
        new_lr = self.lr_scheduler.update(self.episode, avg_makespan)
        if abs(new_lr - self.config.get('meta_lr')) > 1e-6:
            self._update_learning_rates(new_lr)

        # æ›´æ–°æŒ‡æ ‡
        self.metrics.update_metrics(
            makespan=avg_makespan,
            load_balance=avg_load_balance,

            energy=avg_energy,
            throughput=len(workflows) / episode_duration if episode_duration > 0 else 0,
            reward=total_episode_reward
        )

        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        makespan_stability = self.metrics.get_makespan_stability()

        # è®°å½•è®­ç»ƒå†å²
        self.training_history['episode_rewards'].append(float(total_episode_reward))
        self.training_history['makespans'].append(float(avg_makespan) if avg_makespan != float('inf') else 999.0)
        self.training_history['load_balances'].append(float(avg_load_balance))
        self.training_history['makespan_stability'].append(makespan_stability)

        # ğŸ¯ ä¿®å¤ï¼šç»Ÿä¸€çš„æˆåŠŸç‡è®¡ç®—
        makespan_target = self.config.get('makespan_target', 8.0)
        target_success_count = 0
        total_workflows = len(workflows)

        for makespan in valid_makespans:
            if makespan <= makespan_target * 1.2:  # 120%å®¹å·®å†…ç®—æˆåŠŸ
                target_success_count += 1

        target_success_rate = target_success_count / total_workflows if total_workflows > 0 else 0

        # ğŸ¯ ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º
        if self.episode % 15 == 0:
            target_gap = avg_makespan - makespan_target if avg_makespan != float('inf') else float('inf')

            print(f"INFO: Episode {self.episode} - "
                  f"Makespan: {avg_makespan:.3f} (Target: {makespan_target}, Gap: {target_gap:.3f}), "
                  f"Reward: {total_episode_reward:.1f}, "
                  f"Success Rate: {target_success_rate:.1%}, "
                  f"Stability: {makespan_stability:.3f}")

            # MOD: ç»Ÿä¸€æ•°å€¼ä¿æŠ¤
            def _finite(x, default=0.0, allow_inf=False):
                if x is None:
                    return default
                try:
                    if np.isnan(x):
                        return default
                    if not allow_inf and (x == float('inf') or x == float('-inf')):
                        return default
                    return x
                except Exception:
                    return default

            result_dict = {
                'episode': self.episode,
                'total_reward': float(_finite(total_episode_reward)),
                'avg_makespan': float(_finite(avg_makespan, default=float('inf'), allow_inf=True)),
                'avg_load_balance': float(_finite(avg_load_balance)),
                'episode_duration': float(_finite(episode_duration)),
                'workflows_processed': len(workflows),
                'training_losses': training_losses,
                'success_rate': float(_finite(target_success_rate)),
                'basic_completion_rate': float(_finite(len(valid_makespans) / len(workflows) if workflows else 0.0)),
                'makespan_stability': float(_finite(makespan_stability)),
                'smoothed_makespan': float(
                    _finite(self.metrics.get_smoothed_makespan(), default=float('inf'), allow_inf=True)),
                'stored_experiences': int(_finite(stored_count, default=0)),
                'trend_direction': self.metrics.get_trend_direction(),
                'should_stop': bool(should_stop),
                'current_lr': float(_finite(new_lr)),
                'convergence_status': self.convergence_monitor.get_status(),
                'makespan_target': float(_finite(makespan_target)),
                'target_achievement_rate': float(_finite(target_success_rate))
            }
            return result_dict



        return {
            'episode': self.episode,
            'total_reward': float(total_episode_reward),
            'avg_makespan': float(avg_makespan) if avg_makespan != float('inf') else float('inf'),
            'avg_load_balance': float(avg_load_balance),
            'episode_duration': float(episode_duration),
            'workflows_processed': len(workflows),
            'training_losses': training_losses,
            'success_rate': float(target_success_rate),  # ğŸ¯ ä¿®å¤ï¼ä½¿ç”¨ç›®æ ‡åŸºç¡€çš„æˆåŠŸç‡
            'basic_completion_rate': len(valid_makespans) / len(workflows) if workflows else 0,
            'makespan_stability': float(makespan_stability),
            'smoothed_makespan': float(self.metrics.get_smoothed_makespan()),
            'stored_experiences': stored_count,
            'trend_direction': self.metrics.get_trend_direction(),
            'should_stop': should_stop,
            'current_lr': float(new_lr),
            'convergence_status': self.convergence_monitor.get_status(),
            'makespan_target': float(makespan_target),
            'target_achievement_rate': float(target_success_rate)
        }

    def _build_indices(self, tasks):
        id2task = {t.task_id: t for t in tasks}
        pred = {t.task_id: [u for u, _ in getattr(t, 'in_edges', [])] for t in tasks}
        succ = {t.task_id: [v for v, _ in getattr(t, 'out_edges', [])] for t in tasks}
        edge_size = {}
        for t in tasks:
            for v, sz in getattr(t, 'out_edges', []):
                edge_size[(t.task_id, v)] = sz
        return id2task, pred, succ, edge_size

    def _topo_order(self, tasks):
        id2task, pred, succ, _ = self._build_indices(tasks)
        indeg = {tid: len(pred[tid]) for tid in id2task}
        q = [tid for tid, d in indeg.items() if d == 0]
        order = []
        while q:
            u = q.pop(0)
            order.append(u)
            for v in succ[u]:
                indeg[v] -= 1
                if indeg[v] == 0:
                    q.append(v)
        return order if len(order) == len(id2task) else [t.task_id for t in tasks]

    def _preds_info(self, tid, pred, finish, location, edge_size, default_layer='FPGA'):
        infos = []
        for u in pred[tid]:
            ft = finish[u]
            layer_from = location.get(u, default_layer)
            sz = edge_size.get((u, tid), 0.0)
            infos.append((ft, layer_from, sz))
        return infos

    def _node_layer(self, node):
        nt = getattr(node, 'node_type', None)
        return nt.value if hasattr(nt, 'value') else str(nt) if nt else 'CLOUD'

    def schedule_workflow(self, tasks: List, environment) -> Dict:
        """æŒ‰æ‹“æ‰‘é¡ºåºã€ä½¿ç”¨ EFT+CT è°ƒåº¦å•ä¸ªå·¥ä½œæµï¼ˆè®­ç»ƒ/æ¨æ–­é€šç”¨ï¼‰"""
        try:
            environment.reset()
            total_reward = 0.0
            individual_rewards = []
            decisions = []

            # ç¡®ä¿è¯„æµ‹/æ¨æ–­æœŸæœ‰ä¸€ä¸ªæ¸©å’Œçš„ EFT æƒé‡ï¼ˆå¯è¢«å¤–éƒ¨configè¦†ç›–ï¼‰
            if 'eft_weight' not in self.config or self.config.get('eft_weight') is None:
                self.config['eft_weight'] = 0.3

            id2task, pred, succ, edge_size = self._build_indices(tasks)
            order = self._topo_order(tasks)

            finish, location = {}, {}
            task_completion_times = []
            total_energy = 0.0
            failed_tasks = 0

            for idx, tid in enumerate(order):
                task = id2task[tid]
                # 1) å…ƒæ§åˆ¶å™¨é€‰å±‚
                system_state = self._get_system_state(environment)
                selected_cluster, meta_probs = self.meta_controller.select_cluster(system_state)
                if self.config.get('action_smoothing', True):
                    meta_probs = self._simple_smooth_action(meta_probs, 'meta')

                # 2) å­æ§åˆ¶å™¨é€‰èŠ‚ç‚¹ï¼ˆè‹¥è¯¥å±‚æ— èŠ‚ç‚¹ï¼Œå›é€€åˆ°å…¶ä»–å±‚ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åœ¨é€‰èŠ‚ç‚¹å‰å…ˆå‡†å¤‡â€œstart-awareâ€æ‰€éœ€çš„ä¿¡æ¯
                preds_info = self._preds_info(tid, pred, finish, location, edge_size)

                # ä¼°è®¡æ¯ä¸ªå€™é€‰èŠ‚ç‚¹çš„å¯å¼€å§‹/å®Œæˆæ—¶é—´ï¼Œä¾¿äºåº”æ€¥è¦†ç›–æ³•ä¸å¥–åŠ±ä½¿ç”¨
                nodes = self._get_available_nodes(environment, selected_cluster)
                if not nodes:
                    # ç®€å•å›é€€
                    for alt in ['FPGA', 'FOG_GPU', 'CLOUD']:
                        if alt == selected_cluster:
                            continue
                        nodes = self._get_available_nodes(environment, alt)
                        if nodes:
                            selected_cluster = alt
                            break

                if not nodes:
                    failed_tasks += 1
                    individual_rewards.append(-5.0)
                    total_reward += -5.0
                    continue

                # é€‰èŠ‚ç‚¹å‰å…ˆæ„é€ â€œåº”æ€¥è¦†ç›–æ³•â€çš„ cluster_stateï¼ˆç»™å­æ§åˆ¶å™¨ç”¨ï¼‰
                # å–è¯¥å±‚æœŸæœ›ç»´åº¦ï¼ˆä¸å½“å‰å­æ§åˆ¶å™¨è¾“å…¥ä¸€è‡´ï¼‰
                expected_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
                sdim = expected_dims.get(selected_cluster, 6)

                # ä¸ºäº†ç”Ÿæˆ extra ä¸¤ç»´ï¼Œæˆ‘ä»¬éœ€è¦å…ˆè®¡ç®—ä¸€ä¸ªâ€œå€™é€‰EFTâ€çš„è¿‘ä¼¼ç”¨äºå†™å…¥ç¯å¢ƒç¼“å­˜
                # è¿™é‡Œå…ˆåŸºäºå±‚å¹³å‡å¯ç”¨æ—¶é—´å’Œçˆ¶å‡†å¤‡æ—¶é—´ä¼°ä¸ªä¿åº•ï¼ˆå…·ä½“æ¥å£ä»¥ä½ çš„ environment ä¸ºå‡†ï¼‰
                # å¦‚æœ environment.estimate_earliest_times éœ€è¦å…·ä½“èŠ‚ç‚¹ï¼Œæˆ‘ä»¬å…ˆç”¨å±‚ä¸Šæœ€å¿«èŠ‚ç‚¹é¢„ä¼°ä¸€ä¸‹
                try:
                    # æ‰¾åˆ°ä¸€ä¸ªä»£è¡¨èŠ‚ç‚¹åšé¢„ä¼°ï¼ˆæ¯”å¦‚å°±ç”¨å½“å‰å±‚æ‰§è¡Œæ—¶é—´æœ€çŸ­çš„èŠ‚ç‚¹ï¼‰
                    repr_node = min(nodes, key=lambda n: getattr(n, 'completion_time', 0.0))
                except Exception:
                    repr_node = nodes[0]

                # è®¡ç®—ä»£è¡¨èŠ‚ç‚¹ä¸Šçš„ä¼°è®¡æ—¶é—´ï¼ˆEFTæ ¸å¿ƒï¼‰
                try:
                    est_start, exec_time_repr, est_finish = environment.estimate_earliest_times(repr_node, task,
                                                                                                preds_info)
                except Exception:
                    est_start, exec_time_repr, est_finish = 0.0, 1.0, 1.0

                # å°†ä¼°è®¡å€¼å†™å…¥ç¯å¢ƒä¾› _build_start_aware_extra è¯»å–ï¼ˆEFT+åº”æ€¥è¦†ç›–ï¼‰
                try:
                    environment.last_est_start = float(est_start)
                    environment.last_est_finish = float(est_finish)
                except Exception:
                    pass

                # ç”Ÿæˆå­æ§åˆ¶å™¨çŠ¶æ€ï¼ˆEFT+åº”æ€¥è¦†ç›–ï¼šæœ«ä¸¤ç»´ä¸€å®šæ˜¯ est_start/est_finish çš„å½’ä¸€åŒ–è¿‘ä¼¼ï¼‰
                cluster_state = self._get_cluster_state(environment, selected_cluster, task)


                # ç”±å­æ§åˆ¶å™¨é€‰æ‹©èŠ‚ç‚¹
                selected_node, sub_probs = self.sub_controller_manager.select_node_in_cluster(
                    selected_cluster, cluster_state, nodes
                )
                if not selected_node:
                    failed_tasks += 1
                    individual_rewards.append(-5.0)
                    total_reward += -5.0
                    continue

                # 3) è®¡ç®—è¯¥ä»»åŠ¡åœ¨é€‰å®šèŠ‚ç‚¹ä¸Šçš„ EFTï¼ˆå«é€šä¿¡æ—¶é—´ï¼‰
                preds_info = self._preds_info(tid, pred, finish, location, edge_size)
                est_start, exec_time, est_finish = environment.estimate_earliest_times(selected_node, task, preds_info)

                # 4) åˆ†é…ä»»åŠ¡ï¼ˆæ›´æ–° available_time / energyï¼‰
                f, e = environment.assign_task(selected_node, est_start, exec_time, task.memory_requirement)

                finish[tid] = f
                location[tid] = self._node_layer(selected_node)
                total_energy += e
                task_completion_times.append(f)

                # 5) æ¸è¿›å¼å¥–åŠ± + é€šä¿¡æƒ©ç½š + EFT å¼•å¯¼é¡¹
                comm_cost = 0.0
                for ft, lay_from, sz in preds_info:
                    data_mb = max(0.0, float(sz)) if sz is not None else 0.0
                    try:
                        t = environment.get_transmission_time(lay_from, location[tid], data_mb)
                        if t is None or np.isnan(t) or t < 0:
                            t = 0.0
                    except Exception:
                        t = 0.0
                    comm_cost += t

                exec_result = {
                    'execution_time': exec_time,
                    'energy_consumption': e,
                    'quality_score': 1.0,
                    'success': True
                }
                step_reward = self._calculate_progressive_reward(task, exec_result, environment, task_completion_times)
                step_reward -= self.config.get('comm_weight', 0.1) * comm_cost

                # EFT å¼•å¯¼é¡¹ï¼šé¼“åŠ± est_finish æ›´å°ï¼ˆç›¸å¯¹ makespan_target å½’ä¸€åŒ–ï¼‰
                ms_target = float(self.config.get('makespan_target', 1.3))
                eft_weight = float(self.config.get('eft_weight', 0.3))
                norm_eft = float(np.clip(est_finish / max(ms_target, 1e-6), 0.0, 4.0))
                step_reward += - eft_weight * norm_eft  # å…³é”®ä¸€è¡Œ

                total_reward += step_reward
                individual_rewards.append(step_reward)

                decisions.append({
                    'task_id': tid,
                    'selected_cluster': selected_cluster,
                    'selected_node': getattr(selected_node, 'node_id', -1),
                    'execution_time': exec_time,
                    'completion_time': f,
                    'energy_consumption': e,
                    'success': True
                })

            makespan = max(task_completion_times) if task_completion_times else float('inf')
            load_balance = 0.0
            nodes_all = environment.get_available_nodes()
            if nodes_all:
                loads = [n.current_load / n.memory_capacity for n in nodes_all]
                load_balance = max(0, 1.0 - np.std(loads))

            success_rate = (len(order) - failed_tasks) / len(order) if order else 0.0

            return {
                'scheduling_decisions': decisions,
                'total_reward': float(total_reward),
                'makespan': float(makespan) if np.isfinite(makespan) else float('inf'),
                'load_balance': float(load_balance),
                'total_energy': float(total_energy),
                'completed_tasks': len(decisions),
                'failed_tasks': failed_tasks,
                'success_rate': float(success_rate),
                'total_tasks': len(order),
                'individual_rewards': individual_rewards
            }

        except Exception as e:
            print(f"ERROR: schedule_workflow error: {e}")
            return {
                'scheduling_decisions': [],
                'total_reward': -15.0,
                'makespan': float('inf'),
                'load_balance': 0.0,
                'total_energy': 0.0,
                'completed_tasks': 0,
                'failed_tasks': len(tasks),
                'success_rate': 0.0,
                'total_tasks': len(tasks),
                'individual_rewards': [],
            }

    # è¾…åŠ©æ–¹æ³•ä¿æŒä¸å˜ - ç®€åŒ–å±•ç¤º
    def _simple_smooth_action(self, current_action, action_type='meta'):
        """æ•°å€¼æ›´ç¨³å¥çš„åŠ¨ä½œå¹³æ»‘ä¸æ ‡å‡†åŒ–"""
        try:
            alpha = float(self.config.get('action_smoothing_alpha', 0.15))
            prev = self.previous_actions.get(action_type)
            cur = np.asarray(current_action, dtype=np.float32)

            # ä¸€é˜¶å¹³æ»‘
            if prev is not None and len(prev) == len(cur):
                smoothed = alpha * cur + (1.0 - alpha) * prev
            else:
                smoothed = cur

            # éè´Ÿè£å‰ªä¸å½’ä¸€åŒ–ï¼ˆepsilon é˜²å…¨é›¶ï¼‰
            smoothed = np.clip(smoothed, 0.0, None)
            s = float(np.sum(smoothed))
            if s <= 1e-8:
                smoothed = np.ones_like(smoothed, dtype=np.float32) / max(1, len(smoothed))
            else:
                smoothed = smoothed / s

            self.previous_actions[action_type] = smoothed.copy()
            return smoothed
        except Exception as e:
            print(f"WARNING: Action smoothing error: {e}")
            return np.asarray(current_action, dtype=np.float32)

    def _get_system_state(self, environment):
        """è·å–ç³»ç»ŸçŠ¶æ€ï¼ˆé•¿åº¦å¯¹é½ä¿æŠ¤ï¼‰ï¼ˆMODï¼‰"""
        dim = int(self.config.get('meta_state_dim', 15))
        try:
            if hasattr(environment, 'get_system_state'):
                state = environment.get_system_state()
            else:
                state = np.random.random(dim).astype(np.float32)
        except Exception as e:
            print(f"WARNING: Failed to get system state: {e}")
            state = np.random.random(dim).astype(np.float32)

        state = np.asarray(state, dtype=np.float32).flatten()
        if len(state) < dim:
            pad = np.zeros(dim - len(state), dtype=np.float32)
            state = np.concatenate([state, pad], axis=0)
        elif len(state) > dim:
            state = state[:dim]
        return state

    def _get_cluster_state_legacy(self, environment, cluster_type):
        """è·å–é›†ç¾¤çŠ¶æ€ï¼ˆé•¿åº¦å¯¹é½ + æ³¨å…¥ start-aware è¿‘ä¼¼ç‰¹å¾ï¼Œç¯å¢ƒæ— æ¥å£æ—¶ç½®0ï¼‰"""
        # å­æ§åˆ¶å™¨å½“å‰æœŸæœ›ç»´åº¦ï¼ˆä¸ OptimizedSubControllerManager é…ç½®å¯¹é½ï¼‰
        expected_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
        dim = expected_dims.get(cluster_type, 6)


        # 1) è·å–ç¯å¢ƒåŸå§‹ cluster state
        try:
            if hasattr(environment, 'get_cluster_state'):
                base_state = environment.get_cluster_state(cluster_type)
            else:
                base_state = np.random.random(dim).astype(np.float32)
        except Exception:
            base_state = np.random.random(dim).astype(np.float32)

        base_state = np.asarray(base_state, dtype=np.float32).flatten()

        # 2) æ³¨å…¥ start-aware çš„ä¸¤ä¸ªè¿‘ä¼¼ç‰¹å¾ï¼ˆæ— æ¥å£åˆ™ä¸º 0ï¼‰
        def safe_fetch(method_name):
            try:
                fn = getattr(environment, method_name, None)
                if callable(fn):
                    return float(fn(cluster_type))
            except Exception:
                pass
            return 0.0

        parents_ready_on_layer = safe_fetch('get_cluster_parents_ready_time')  # è¿‘ä¼¼çš„çˆ¶æ•°æ®å°±ç»ªæ—¶é—´
        avg_node_available = safe_fetch('get_cluster_avg_available_time')  # è¿‘ä¼¼çš„è¯¥å±‚å¹³å‡å¯ç”¨æ—¶é—´

        extra = np.array([parents_ready_on_layer, avg_node_available], dtype=np.float32)

        # 3) æ‹¼æ¥ååšé•¿åº¦å¯¹é½ï¼ˆæˆªæ–­æˆ–å¡«å……ï¼‰ï¼Œä»¥ä¸ç ´åå­æ§åˆ¶å™¨ç½‘ç»œçš„è¾“å…¥ç»´åº¦
        state = np.concatenate([base_state, extra], axis=0)

        if len(state) < dim:
            pad = np.zeros(dim - len(state), dtype=np.float32)
            state = np.concatenate([state, pad], axis=0)
        elif len(state) > dim:
            state = state[:dim]

        return state

    def _get_available_nodes(self, environment, cluster_type):
        """è·å–å¯ç”¨èŠ‚ç‚¹"""
        try:
            if hasattr(environment, 'get_available_nodes'):
                nodes = environment.get_available_nodes(cluster_type)
                return nodes if nodes else []
            else:
                return self._create_mock_nodes(cluster_type)
        except Exception as e:
            return self._create_mock_nodes(cluster_type)

    def _cluster_fallback(self, selected_cluster, available_nodes, environment):
        """é›†ç¾¤å›é€€ç­–ç•¥"""
        if available_nodes:
            return selected_cluster, available_nodes

        # ç®€å•å›é€€
        for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
            if cluster_type != selected_cluster:
                alt_nodes = self._get_available_nodes(environment, cluster_type)
                if alt_nodes:
                    return cluster_type, alt_nodes

        return selected_cluster, []

    def _simple_topological_sort(self, tasks: List) -> List:
        """ç®€åŒ–çš„æ‹“æ‰‘æ’åº"""
        try:
            if hasattr(tasks[0], 'dependencies') if tasks else False:
                sorted_tasks = []
                remaining_tasks = tasks.copy()

                while remaining_tasks:
                    ready_tasks = []
                    for task in remaining_tasks:
                        deps = getattr(task, 'dependencies', [])
                        if not deps or all(dep not in [t.task_id for t in remaining_tasks] for dep in deps):
                            ready_tasks.append(task)

                    if not ready_tasks:
                        sorted_tasks.extend(remaining_tasks)
                        break

                    sorted_tasks.extend(ready_tasks)
                    for task in ready_tasks:
                        remaining_tasks.remove(task)

                return sorted_tasks
            else:
                return tasks
        except Exception as e:
            print(f"WARNING: Topological sort error: {e}")
            return tasks

    def _collect_experiences(self, scheduling_result, meta_experiences, sub_experiences):
        """ç»éªŒæ”¶é›†ï¼ˆæ•°å€¼æ›´ç¨³å¥ï¼‰ï¼ˆMODï¼‰"""
        try:
            decisions = scheduling_result.get('scheduling_decisions', [])
            total_reward = float(scheduling_result.get('total_reward', 0))

            for decision in decisions:
                if decision.get('success', False):
                    cluster_type = decision.get('selected_cluster')
                    if cluster_type and cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
                        state_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
                        action_dims = {'FPGA': 2, 'FOG_GPU': 3, 'CLOUD': 2}

                        sdim = state_dims.get(cluster_type, 6)
                        adim = action_dims.get(cluster_type, 2)

                        # è¿‘ä¼¼çŠ¶æ€ï¼ˆçœŸå®ç»´åº¦ï¼‰
                        state = np.random.random(sdim).astype(np.float32)

                        # è¿‘ä¼¼åŠ¨ä½œåˆ†å¸ƒï¼ˆå½’ä¸€åŒ–ï¼‰
                        action = np.random.random(adim).astype(np.float32)
                        asum = np.sum(action)
                        if asum <= 1e-8:
                            action = np.ones(adim, dtype=np.float32) / adim
                        else:
                            action = action / asum

                        # æ­¥éª¤å¥–åŠ±ï¼šä½¿ç”¨æ‰§è¡Œæ—¶é—´çš„è´Ÿç›¸å…³è¿‘ä¼¼
                        exec_time = float(decision.get('execution_time', 1.0))
                        reward = np.clip(2.5 - exec_time, -2.0, 2.5).item()

                        next_state = np.random.random(sdim).astype(np.float32)
                        done = False
                        sub_experiences[cluster_type].append((state, action, reward, next_state, done))

                    # Metaç»éªŒï¼ˆç»Ÿä¸€ç»´åº¦ï¼‰
                    meta_dim = int(self.config.get('meta_state_dim', 15))
                    meta_state = np.random.random(meta_dim).astype(np.float32)
                    meta_action = np.random.random(3).astype(np.float32)
                    meta_action = meta_action / max(1e-8, np.sum(meta_action))

                    meta_reward = float(total_reward / max(len(decisions), 1))
                    meta_reward = float(np.clip(meta_reward, -8.0, 30.0))  # è£å‰ªï¼ˆMODï¼‰
                    meta_next_state = np.random.random(meta_dim).astype(np.float32)
                    meta_done = False

                    meta_experiences.append((meta_state, meta_action, meta_reward, meta_next_state, meta_done))

        except Exception as e:
            print(f"WARNING: Experience collection error: {e}")

    def _store_experiences_to_buffer(self, meta_experiences, sub_experiences):
        """ç»éªŒå­˜å‚¨"""
        stored_count = 0

        try:
            for exp in meta_experiences[-20:]:
                if len(exp) >= 5:
                    success = self.replay_buffer.push_meta(*exp)
                    if success:
                        stored_count += 1

            for cluster_type, experiences in sub_experiences.items():
                for exp in experiences[-10:]:
                    if len(exp) >= 5:
                        success = self.replay_buffer.push_sub(cluster_type, *exp)
                        if success:
                            stored_count += 1

            return stored_count

        except Exception as e:
            print(f"WARNING: Experience storage error: {e}")
            return 0

    def _update_networks(self) -> Dict:
        """ç½‘ç»œæ›´æ–°"""
        losses = {'meta_critic': 0, 'meta_actor': 0}

        try:
            self.update_counter += 1

            # æ›´æ–°å…ƒæ§åˆ¶å™¨
            if self.replay_buffer.can_sample(self.config['batch_size']):
                states, actions, rewards, next_states, dones, weights = self.replay_buffer.sample_meta(
                    self.config['batch_size']
                )

                if len(states) > 0:
                    try:
                        critic_loss, actor_loss = self.meta_controller.update(
                            states, actions, rewards, next_states, dones
                        )

                        if not (np.isnan(critic_loss) or np.isnan(actor_loss)):
                            losses['meta_critic'] = float(critic_loss)
                            losses['meta_actor'] = float(actor_loss)

                    except Exception as e:
                        print(f"WARNING: Meta controller update error: {e}")

            # æ›´æ–°å­æ§åˆ¶å™¨
            for cluster_type in ['FPGA', 'FOG_GPU', 'CLOUD']:
                batch_size = max(4, self.config['batch_size'] // 8)
                if self.replay_buffer.can_sample_sub(cluster_type, batch_size):
                    try:
                        states, actions, rewards, next_states, dones, weights = self.replay_buffer.sample_sub(
                            cluster_type, batch_size
                        )

                        if len(states) > 0:
                            critic_loss, actor_loss = self.sub_controller_manager.update_sub_controller(
                                cluster_type, states, actions, rewards, next_states, dones
                            )

                            if not (np.isnan(critic_loss) or np.isnan(actor_loss)):
                                losses[f'{cluster_type.lower()}_critic'] = float(critic_loss)
                                losses[f'{cluster_type.lower()}_actor'] = float(actor_loss)

                    except Exception as e:
                        print(f"WARNING: Sub controller update error ({cluster_type}): {e}")

        except Exception as e:
            print(f"WARNING: Network update error: {e}")

        return losses

    def _calculate_performance_metrics(self, task_completion_times, scheduling_decisions, environment):
        try:
            if task_completion_times:
                valid_times = [t for t in task_completion_times if np.isfinite(t)]
                makespan = max(valid_times) if valid_times else float('inf')
            else:
                makespan = float('inf')
            total_energy = sum(d.get('energy_consumption', 0) for d in scheduling_decisions)
            # å¯é€‰ï¼šè‹¥éœ€èŠ‚ç‚¹ç´¯è®¡èƒ½è€—
            # for nl in environment.nodes.values():
            #     for n in nl:
            #         total_energy += getattr(n, 'energy_accum', 0.0)
            load_balance = 0.0
            nodes_all = environment.get_available_nodes()
            if nodes_all:
                loads = [n.current_load / n.memory_capacity for n in nodes_all]
                load_balance = max(0, 1.0 - np.std(loads))
            return makespan, load_balance, total_energy
        except Exception:
            return float('inf'), 0.0, 0.0

    def _execute_task(self, task, node, environment, pre_transmission_time: float = 0.0,
                      node_ready_time: dict = None, parents_ready_time: float = 0.0) -> Dict:
        """ä»»åŠ¡æ‰§è¡Œï¼ˆå«å¯é€‰çš„å‰ç½®ä¼ è¾“æ—¶é—´ä¸èŠ‚ç‚¹æ—¶é—´çº¿ï¼‰"""
        try:
            memory_requirement = float(getattr(task, 'memory_requirement', 10.0))
            computation_requirement = float(getattr(task, 'computation_requirement', 1.0))

            # è¯†åˆ«é›†ç¾¤ç±»å‹
            node_type = getattr(node, 'node_type', None)
            if hasattr(node_type, 'value'):
                cluster_type = node_type.value
            else:
                cluster_type = str(node_type) if node_type else 'CLOUD'

            # è®¡ç®—æ‰§è¡Œæ—¶é—´ï¼ˆä¿ç•™ä½ åŸæ¥çš„è¿‘ä¼¼é€»è¾‘ï¼‰
            base_times = {'FPGA': 0.8, 'FOG_GPU': 0.5, 'CLOUD': 0.3}
            base_time = base_times.get(cluster_type, 0.5)
            execution_time = base_time * computation_requirement

            if hasattr(node, 'current_load') and hasattr(node, 'memory_capacity'):
                load_ratio = 0.0
                if node.memory_capacity > 0:
                    load_ratio = node.current_load / node.memory_capacity
                execution_time *= (1.0 + 0.3 * load_ratio)

            execution_time = float(max(0.05, execution_time))

            # èŠ‚ç‚¹æ—¶é—´çº¿ä¸çˆ¶ä¾èµ–å‡†å¤‡æ—¶é—´ï¼ˆå¯é€‰å¢å¼ºï¼‰
            node_start_ready = 0.0
            if node_ready_time is not None and hasattr(node, 'node_id'):
                node_start_ready = float(node_ready_time.get(node.node_id, 0.0))

            parents_ready_time = float(parents_ready_time or 0.0)
            global_time = float(getattr(environment, 'current_time', 0.0))

            # å¼€å§‹æ—¶é—´ï¼šå–æ‰€æœ‰å°±ç»ªæ—¶åˆ»çš„æœ€å¤§å€¼ï¼Œå†å åŠ å‰ç½®ä¼ è¾“æ—¶é—´
            start_time = max(global_time, node_start_ready, parents_ready_time) + float(pre_transmission_time)

            completion_time = start_time + execution_time

            # æ›´æ–°å…¨å±€ä¸èŠ‚ç‚¹æ—¶é—´çº¿
            if node_ready_time is not None and hasattr(node, 'node_id'):
                node_ready_time[node.node_id] = completion_time

            if hasattr(environment, 'current_time'):
                # ä¿æŒå…¨å±€æ—¶é’Ÿä¸ºâ€œå·²çŸ¥å®Œæˆæ—¶åˆ»â€çš„æœ€å¤§å€¼ï¼ˆå…è®¸å¹¶è¡Œï¼‰
                environment.current_time = max(environment.current_time, completion_time)

            # èƒ½è€—ï¼ˆä¿æŒä½ çš„åŸé€»è¾‘ï¼‰
            base_energy = {'FPGA': 12, 'FOG_GPU': 25, 'CLOUD': 45}
            energy_per_second = base_energy.get(cluster_type, 25)
            energy_consumption = float(energy_per_second * execution_time)

            # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
            if hasattr(environment, 'update_node_load') and hasattr(node, 'node_id'):
                try:
                    environment.update_node_load(node.node_id, memory_requirement)
                except Exception:
                    pass

            return {
                'execution_time': float(execution_time),
                'transmission_time': float(pre_transmission_time),  # æ–°å¢ï¼šä¾¿äºè¯Šæ–­
                'start_time': float(start_time),
                'completion_time': float(completion_time),
                'energy_consumption': float(energy_consumption),
                'quality_score': 1.0,
                'success': True
            }

        except Exception as e:
            print(f"WARNING: Task execution error: {e}")
            return {
                'execution_time': float('inf'),
                'transmission_time': 0.0,
                'start_time': float('inf'),
                'completion_time': float('inf'),
                'energy_consumption': 0.0,
                'quality_score': 0.0,
                'success': False,
                'failure_reason': 'execution_error'
            }
    def _create_mock_nodes(self, cluster_type):
        """åˆ›å»ºæ¨¡æ‹ŸèŠ‚ç‚¹"""
        class SimpleMockNode:
            def __init__(self, node_id, cluster_type):
                self.node_id = node_id
                self.cluster_type = cluster_type
                self.memory_capacity = 100
                self.current_load = 0
                self.availability = True

        node_counts = {'FPGA': 4, 'FOG_GPU': 3, 'CLOUD': 1}
        count = node_counts.get(cluster_type, 2)
        return [SimpleMockNode(i, cluster_type) for i in range(count)]

    # æ¥å£æ–¹æ³•
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            self.meta_controller.save_weights(filepath)
            self.sub_controller_manager.save_all_weights(filepath)
            print(f"INFO: Meta controller weights saved to optimized_model_result\\{filepath.split('/')[-1]}\\final_optimized_model")
            print(f"INFO: FPGA weights saved successfully")
            print(f"INFO: FOG_GPU weights saved successfully")
            print(f"INFO: CLOUD weights saved successfully")
            print(f"INFO: Models saved to optimized_model_result\\{filepath.split('/')[-1]}\\final_optimized_model")
        except Exception as e:
            print(f"ERROR: Model save error: {e}")

    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.meta_controller.load_weights(filepath)
            self.sub_controller_manager.load_all_weights(filepath)
            print(f"INFO: Models loaded from {filepath}")
        except Exception as e:
            print(f"ERROR: Model load error: {e}")

    def get_training_summary(self) -> Dict:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        try:
            recent_rewards = self.training_history['episode_rewards'][-50:]
            recent_makespans = self.training_history['makespans'][-50:]
            recent_load_balances = self.training_history['load_balances'][-50:]
            recent_stability = self.training_history['makespan_stability'][-50:]

            return {
                'total_episodes': self.episode,
                'average_reward_recent_50': np.mean(recent_rewards) if recent_rewards else 0,
                'average_makespan_recent_50': np.mean(recent_makespans) if recent_makespans else 0,
                'average_load_balance_recent_50': np.mean(recent_load_balances) if recent_load_balances else 0,
                'average_stability_recent_50': np.mean(recent_stability) if recent_stability else 0,
                'smoothed_makespan': self.metrics.get_smoothed_makespan(),
                'trend_direction': self.metrics.get_trend_direction(),
                'training_history': self.training_history,
                'convergence_status': self.convergence_monitor.get_status(),
                'is_converged': self.is_converged,
                'makespan_target': self.config.get('makespan_target', 8.0),
                'best_makespan_achieved': self.best_performance
            }
        except Exception as e:
            print(f"WARNING: Training summary error: {e}")
            return {
                'total_episodes': self.episode,
                'training_history': self.training_history,
                'is_converged': False
            }


    def get_optimization_status(self) -> Dict:
        """è·å–ä¼˜åŒ–çŠ¶æ€æŠ¥å‘Š"""
        try:
            recent_makespans = self.training_history['makespans'][-30:] if self.training_history['makespans'] else []

            if len(recent_makespans) >= 10:
                makespan_trend = np.polyfit(range(len(recent_makespans)), recent_makespans, 1)[0]
                makespan_variance = np.var(recent_makespans)
            else:
                makespan_trend = 0
                makespan_variance = 0

            makespan_target = self.config.get('makespan_target', 8.0)
            target_achievement = 0.0
            if recent_makespans:
                best_recent = min(recent_makespans)
                if best_recent <= makespan_target:
                    target_achievement = 1.0
                else:
                    target_achievement = max(0.0, 1.0 - (best_recent - makespan_target) / makespan_target)

            return {
                'episode': self.episode,
                'makespan_trend': float(makespan_trend),
                'makespan_variance': float(makespan_variance),
                'stability_score': self.metrics.get_makespan_stability(),
                'trend_direction': self.metrics.get_trend_direction(),
                'buffer_utilization': len(self.replay_buffer) / self.config['memory_capacity'],
                'optimization_progress': {
                    'improving': makespan_trend < -0.05,
                    'stable': makespan_variance < 1.0,
                    'converged': self.is_converged
                },
                'current_lr': self.lr_scheduler.current_lr,
                'convergence_monitor': self.convergence_monitor.get_status(),
                'makespan_target': makespan_target,
                'target_achievement_rate': target_achievement,
                'best_makespan': self.best_performance
            }

        except Exception as e:
            print(f"WARNING: Optimization status error: {e}")
            return {
                'episode': self.episode,
                'is_converged': False,
                'makespan_target': self.config.get('makespan_target', 8.0)
            }

    def _build_start_aware_extra(self, env, cluster_id, task_info):
        """
        ç”Ÿæˆä¸HEFT/EFTç›¸å…³çš„ä¸¤ä¸ªè½»é‡ç‰¹å¾ï¼Œç”¨äºå¼•å¯¼å­æ§åˆ¶å™¨ï¼š
        - extra[0]: è¯¥é›†ç¾¤/è®¾å¤‡çš„ earliest_startï¼ˆæˆ–é˜Ÿåˆ—å°±ç»ªæ—¶é—´ï¼‰çš„å½’ä¸€åŒ–è¿‘ä¼¼
        - extra[1]: è¯¥ä»»åŠ¡åœ¨è¯¥é›†ç¾¤å®Œæˆçš„ est_finish çš„å½’ä¸€åŒ–è¿‘ä¼¼
        å…·ä½“æ•°å€¼è¯·ä¸ç¯å¢ƒä¸­å·²æœ‰çš„ä¼°è®¡å‡½æ•°ä¿æŒä¸€è‡´ã€‚è¿™é‡Œç»™å‡ºä¸€ä¸ªå‚è€ƒå®ç°ï¼Œä½ å¯æ›¿æ¢ä¸ºé¡¹ç›®å†…çš„çœŸå®ä¼°è®¡ã€‚
        """
        # ç¤ºä¾‹ï¼šä»ç¯å¢ƒè·å–ä¼°è®¡å€¼ï¼ˆè¯·æ›¿æ¢ä¸ºä½ ä»¬å·²æœ‰çš„ä¼°è®¡æ¥å£ï¼‰
        # est_start, est_finish = env.estimate_earliest_times(cluster_id, task_info)
        # ä¸ºé¿å…å¤–éƒ¨ä¾èµ–ï¼Œè¿™é‡Œä¿åº•å¤„ç†ä¸º0ï¼›ä½ åº”è°ƒç”¨å·²æœ‰çš„ä¼°è®¡å‡½æ•°ã€‚
        est_start = getattr(env, 'last_est_start', 0.0)
        est_finish = getattr(env, 'last_est_finish', 0.0)

        # å½’ä¸€åŒ–ï¼šç›¸å¯¹ makespan_targetï¼Œé¿å…å°ºåº¦è¿‡å¤§
        ms_target = float(self.config.get('makespan_target', 1.3))
        extra0 = np.clip(est_start / max(ms_target, 1e-6), 0.0, 4.0)
        extra1 = np.clip(est_finish / max(ms_target, 1e-6), 0.0, 4.0)
        return np.array([extra0, extra1], dtype=np.float32)

    def _get_base_cluster_state(self, env, cluster_id, task_info):
        try:
            if hasattr(env, 'get_cluster_observation') and callable(env.get_cluster_observation):
                state = env.get_cluster_observation(cluster_id, task_info)
            elif hasattr(env, 'get_cluster_state') and callable(env.get_cluster_state):
                state = env.get_cluster_state(cluster_id)
            else:
                expected_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
                dim = expected_dims.get(cluster_id, 6)
                state = np.zeros(dim, dtype=np.float32)
        except Exception:
            expected_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
            dim = expected_dims.get(cluster_id, 6)
            state = np.zeros(dim, dtype=np.float32)
        return np.asarray(state, dtype=np.float32)

    def _get_cluster_state(self, env, cluster_type: str, task_info=None) -> np.ndarray:
        """
        ä¸ç¯å¢ƒç»´åº¦å¯¹é½ï¼ˆFPGA=6/FOG=8/CLOUD=6ï¼‰ï¼Œä¸å¢ç»´ã€‚
        è¦†ç›–æœ€åä¸¤ç»´ä¸º start-aware ç‰¹å¾ï¼šest_start_normã€est_finish_normã€‚
        """
        # 1) è·å–ç¯å¢ƒç»™å®šçš„ cluster_state
        try:
            base_state = env.get_cluster_state(cluster_type)
        except Exception:
            fallback_dims = {'FPGA': 6, 'FOG_GPU': 8, 'CLOUD': 6}
            base_state = np.zeros(fallback_dims.get(cluster_type, 6), dtype=np.float32)

        state = np.asarray(base_state, dtype=np.float32).flatten()

        # 2) è®¡ç®— start-aware ä¸¤ç»´ï¼ˆä» environment.last_est_* è¯»å–ï¼›schedule_workflow å·²å†™å…¥ï¼‰
        extra = self._build_start_aware_extra(env, cluster_type, task_info)  # [est_start_norm, est_finish_norm]

        # 3) è¦†ç›–æœ€åä¸¤ç»´ï¼›è‹¥é•¿åº¦ä¸è¶³2åˆ™å…ˆpadåˆ°2
        if state.size < 2:
            state = np.concatenate([state, np.zeros(2 - state.size, dtype=np.float32)], axis=0)

        # å¯é€‰ï¼šè‹¥æƒ³ä¿ç•™ç¯å¢ƒæœ«ä¸¤ç»´ï¼ˆavg_loadã€cluster_efficiencyï¼‰ï¼Œå¯å…ˆæŠŠå®ƒä»¬å‰ç§»ï¼š
        # if state.size >= 4:
        #     state[-4:-2] = state[-2:]

        state[-2:] = extra[:2]
        return state.astype(np.float32)

    # ä½ ä»¬å­æ§åˆ¶å™¨é€‰æ‹©åŠ¨ä½œæ—¶ï¼Œè°ƒç”¨æ–¹å¼ç¤ºä¾‹ï¼ˆè¯·å°† expected_dim ä¼ å…¥ä¸ºé¡¹ç›®é‡Œè®¾ç½®çš„é‚£ä¸€å±‚stateç»´åº¦ï¼‰ï¼š
    def get_action_for_cluster(self, env, cluster_id, task_info, expected_dim, controller):
        state = self._get_cluster_state(env, cluster_id, task_info, expected_dim)
        action = controller.select_action(state)
        return action
